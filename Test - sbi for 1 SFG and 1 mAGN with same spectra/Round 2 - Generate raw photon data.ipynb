{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Current working dir  …/DGRB Scripts/Test - sbi for 1 SFG and 1 mAGN…\n",
    "# parent[0] → …/DGRB Scripts\n",
    "# parent[1] → …/home/users/ids29           ← where “DGRB/” lives\n",
    "package_path = Path.cwd().parents[1] / \"DGRB\"   # /home/users/ids29/DGRB\n",
    "\n",
    "sys.path.insert(0, str(package_path))           # make it import-able\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aegis\n",
    "import numpy as np\n",
    "import healpy as hp\n",
    "import torch\n",
    "import pickle as pk\n",
    "from astropy import units as u\n",
    "from astropy import constants as c\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "import os\n",
    "from sbi.inference import SNLE, SNPE#, prepare_for_sbi, simulate_for_sbi\n",
    "from sbi import utils as utils\n",
    "from sbi import analysis as analysis\n",
    "# from sbi.inference.base import infer\n",
    "from getdist import plots, MCSamples\n",
    "import pickle\n",
    "from scipy.stats import norm\n",
    "from scipy.integrate import quad, simpson\n",
    "from joblib import Parallel, delayed\n",
    "from sbi.neural_nets import posterior_nn\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "grains=1000\n",
    "num_simulations = 1000\n",
    "num_workers = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_range = [[], []]\n",
    "abundance_luminosity_and_spectrum_list = []\n",
    "source_class_list = []\n",
    "parameter_names = []\n",
    "energy_range = [1000, 100000] #MeV\n",
    "energy_range_gen = [energy_range[0]*0.5, energy_range[1]*18]\n",
    "max_radius = 8.5 + 20*2 #kpc\n",
    "exposure = 2000*10*0.2 #cm^2 yr\n",
    "flux_cut = 1e-9 #photons/cm^2/s\n",
    "angular_cut = np.pi #10*u.deg.to('rad') #degrees\n",
    "angular_cut_gen = np.pi #angular_cut*1.5\n",
    "lat_cut = 0 #2*u.deg.to('rad') #degrees\n",
    "lat_cut_gen = lat_cut*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_cosmology = 'Planck18'\n",
    "z_range = [0, 14]\n",
    "luminosity_range = 10.0**np.array([37, 50]) # Minimum value set by considering Andromeda distance using Fermi as benchmark and receiving 0.1 photon at detector side\n",
    "my_AEGIS = aegis.aegis(abundance_luminosity_and_spectrum_list, source_class_list, parameter_range, energy_range, luminosity_range, max_radius, exposure, angular_cut, lat_cut, flux_cut, energy_range_gen=energy_range_gen, cosmology = my_cosmology, z_range = z_range, verbose = False)\n",
    "my_AEGIS.angular_cut_gen, my_AEGIS.lat_cut_gen = angular_cut_gen, lat_cut_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gamma_SFG = 2.2\n",
    "gamma_energy_bounds = energy_range_gen  # in MeV\n",
    "E_photon_MeV_SFG = ((-Gamma_SFG + 1) / (-Gamma_SFG + 2) *\n",
    "                (gamma_energy_bounds[1]**(-Gamma_SFG + 2) - gamma_energy_bounds[0]**(-Gamma_SFG + 2)) /\n",
    "                (gamma_energy_bounds[1]**(-Gamma_SFG + 1) - gamma_energy_bounds[0]**(-Gamma_SFG + 1))) # in MeV\n",
    "E_photon_SFG = E_photon_MeV_SFG * 1.60218e-6  # erg\n",
    "\n",
    "Gamma_mAGN = 2.2 # enforced by user to match SFG spectrum (actually should 2.25)\n",
    "gamma_energy_bounds = energy_range_gen  # in MeV\n",
    "E_photon_MeV_mAGN = ((-Gamma_mAGN + 1) / (-Gamma_mAGN + 2) *\n",
    "                (gamma_energy_bounds[1]**(-Gamma_mAGN + 2) - gamma_energy_bounds[0]**(-Gamma_mAGN + 2)) /\n",
    "                (gamma_energy_bounds[1]**(-Gamma_mAGN + 1) - gamma_energy_bounds[0]**(-Gamma_mAGN + 1))) # MeV\n",
    "E_photon_mAGN = E_photon_MeV_mAGN * 1.60218e-6  # erg\n",
    "\n",
    "res = int(1e4)\n",
    "log_LIRs = np.linspace(-5, 25, res)\n",
    "log_L5Gs = np.linspace(20, 55, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ZL_SFG1(z, l, params):\n",
    "\n",
    "\n",
    "    log_PhiStar = params[0]\n",
    "    Phi_star = 10**log_PhiStar\n",
    "\n",
    "    l_erg = l * E_photon_SFG # erg/s\n",
    "    LFs = np.zeros_like(l)\n",
    "\n",
    "    def Phi_IR(log_LIR): #log_LIR = log_10(L_IR / solar_luminosity) # unitless\n",
    "\n",
    "        # from Table 8 in Gruppioni et al.\n",
    "        # Phi_star = 10**(-2.08) # Mpc^{-3} dex^{-1}\n",
    "        Lstar = 10**(9.46) # Solar luminosity\n",
    "        alpha = 1.00\n",
    "        sigma = 0.50\n",
    "\n",
    "        LIR = 10**log_LIR # solar luminosity\n",
    "\n",
    "        Phi_IR = Phi_star * (LIR / Lstar)**(1 - alpha) * np.exp(-1 / (2 * sigma**2) * (np.log10(1 + LIR / Lstar))**2) # from Gruppioni paper eqn (3)  \t\n",
    "\n",
    "        return Phi_IR\n",
    "\n",
    "    def PDF_log_Lgamma_given_log_LIR(log_LIR, log_Lgamma): #log_LIR = log_10(L_IR / solar_luminosity) # unitless\n",
    "        LIR_solar_luminosity = 10**log_LIR # Solar luminosity\n",
    "        L_IR_erg_second = LIR_solar_luminosity * 3.826e33 # erg/s\n",
    "\n",
    "        a = 1.09\n",
    "        g = 40.8\n",
    "        sigma_SF = 0.202 \n",
    "\n",
    "        mean = g + a * np.log10(L_IR_erg_second / 1e45)\n",
    "        std = sigma_SF\n",
    "\n",
    "        return norm.pdf(log_Lgamma, loc=mean, scale=std)\n",
    "\n",
    "    def integrand(PhiIR_of_logLIRs, log_LIRs, log_Lgamma):\n",
    "        return PhiIR_of_logLIRs * PDF_log_Lgamma_given_log_LIR(log_LIRs, log_Lgamma)\n",
    "\n",
    "    PhiIR_of_logLIRs = Phi_IR(log_LIRs)\n",
    "\n",
    "    for i in range(LFs.shape[0]):\n",
    "        for j in range(LFs.shape[1]):\n",
    "            LFs[i,j] = simpson(integrand(PhiIR_of_logLIRs, log_LIRs, np.log10(l_erg[i,j])), x=log_LIRs)\n",
    "    return 1e-9 / np.log(10) / l * LFs # LF has spatial units of Mpc^{-3}. We need to convert this to kpc^{-3}. Hence the factor of 1e-9\n",
    "\n",
    "\n",
    "def spec_SFG1(energy, params):\n",
    "    Gamma = 2.2\n",
    "    return energy**(-Gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ZL_mAGN(z, l, params):\n",
    "\n",
    "    log_phi1 = params[1]\n",
    "    phi1 = 10**log_phi1\n",
    "\n",
    "    l_erg = l * E_photon_mAGN # erg/s\n",
    "    LFs = np.zeros_like(l)\n",
    "\n",
    "    def Phi_5G(log_L5G, z): #log_L5G = log_10(L_5GHz / (erg/s)) # unitless\n",
    "        #Output is in Mpc^{-3}\n",
    "\n",
    "        L_5G = 10**log_L5G # erg/s\n",
    "        radio_bandwidth = 4.87e9 # measured in Hz # width of radio band centered around blueshifted frequency of 5GHz \n",
    "        diff_L5G = L_5G / radio_bandwidth * 1e-7 # measured in W/Hz # Converted erg to Joule # luminosity per unit frequency\n",
    "\n",
    "        # Values taken from Table 4 of Yuan 2018 paper. Second row.\n",
    "        p1 = 2.085\n",
    "        p2 = -4.602\n",
    "        z_c = 0.893\n",
    "        k1 = 1.744\n",
    "        e1 = ( (1+z_c)**p1 + (1+z_c)**p2 ) / ( ((1+z_c)/(1+z))**p1 + ((1+z_c)/(1+z))**p2 )\n",
    "        e2 = (1+z)**k1\n",
    "        # phi1 = 10**(-3.749) # Mpc^{-3}\n",
    "        L_star = 10**21.592 # W/Hz\n",
    "        beta = 0.139\n",
    "        gamma = 0.878\n",
    "\n",
    "        # From Yuan 2018 paper equation 21\n",
    "        # Note that this is dN/dV dlog(diff_5G). But this is also equal to dN/dV dlog(L_5G) because the radio bandwidth is fixed.\n",
    "        Phi_5G = e1 * phi1 * ( (diff_L5G / (e2 * L_star))**beta + (diff_L5G / (e2 * L_star))**gamma )**-1\n",
    "\n",
    "        return Phi_5G\n",
    "    \n",
    "\n",
    "    def PDF_log_Lgamma_given_log_L5G(log_L5G, log_Lgamma): #log_L5G = log_10(L_5GHz / (erg/s)) # unitless\n",
    "        L_5GHz = 10**log_L5G # erg/s\n",
    "\n",
    "        b = 0.78\n",
    "        d = 40.78\n",
    "        sigma_mAGN = 0.880\n",
    "\n",
    "        mean = d + b * np.log10(L_5GHz / 1e40)\n",
    "        std = sigma_mAGN\n",
    "\n",
    "        return norm.pdf(log_Lgamma, loc=mean, scale=std)\n",
    "    \n",
    "\n",
    "    def integrand(log_L5G, z, log_Lgamma):\n",
    "        return Phi_5G(log_L5G, z) * PDF_log_Lgamma_given_log_L5G(log_L5G, log_Lgamma)\n",
    "    \n",
    "\n",
    "\n",
    "    for i in range(LFs.shape[0]):\n",
    "        for j in range(LFs.shape[1]):\n",
    "            LFs[i,j] = simpson(integrand(log_L5Gs, z[i,j], np.log10(l_erg[i,j])), x=log_L5Gs)\n",
    "\n",
    "\n",
    "    return 1e-9 / np.log(10) / l * LFs # LF has spatial units of Mpc^{-3}. We need to convert this to kpc^{-3}. Hence the factor of 1e-9\n",
    "\n",
    "\n",
    "\n",
    "def spec_mAGN(energy, params):\n",
    "    Gamma = 2.2 #modified sepctrum to match the SFG spectrum\n",
    "    return energy**(-Gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "als_SFG1 = [ZL_SFG1, spec_SFG1]\n",
    "als_mAGN = [ZL_mAGN, spec_mAGN]\n",
    "my_AEGIS.abun_lum_spec = [als_SFG1, als_mAGN]\n",
    "my_AEGIS.source_class_list = ['extragalactic_isotropic_faint_single_spectrum', 'extragalactic_isotropic_faint_single_spectrum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple simulator with the total number of photons as the summary statistic\n",
    "def simulator(params):\n",
    "\n",
    "    input_params = params.numpy()\n",
    "\n",
    "    source_info = my_AEGIS.create_sources(input_params, grains=grains, epsilon=1e-2)\n",
    "    photon_info = my_AEGIS.generate_photons_from_sources(input_params, source_info, grains=grains) \n",
    "    obs_info = {'psf_fits_path': '../../DGRB/FERMI_files/psf_P8R3_ULTRACLEANVETO_V2_PSF.fits', 'edisp_fits_path': '../../DGRB/FERMI_files/edisp_P8R3_ULTRACLEANVETO_V2_PSF.fits', 'event_type': 'PSF3', 'exposure_map': None}\n",
    "    obs_photon_info = my_AEGIS.mock_observe(photon_info, obs_info)\n",
    "    \n",
    "    return obs_photon_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def manual_simulate_for_sbi(proposal, num_simulations=1000, num_workers=32):\n",
    "#     \"\"\"\n",
    "#     Simulates the model in parallel using joblib.\n",
    "#     Each simulation call samples a parameter from the proposal and passes the index to the simulator.\n",
    "#     \"\"\"\n",
    "#     def run_simulation(i):\n",
    "#         if i % 10 == 0:\n",
    "#             print(f\"i= {i}\")\n",
    "#         # Sample a parameter from the proposal (sbi.utils.BoxUniform has a .sample() method)\n",
    "#         theta_i = proposal.sample()\n",
    "#         photon_info = simulator(theta_i)\n",
    "#         return theta_i, photon_info\n",
    "\n",
    "#     # Run simulations in parallel using joblib.\n",
    "#     results = Parallel(n_jobs=num_workers, timeout=None)(delayed(run_simulation)(i) for i in range(num_simulations))\n",
    "#     theta_list, photon_info_list = zip(*results)\n",
    "\n",
    "#     theta_tensor = torch.stack(theta_list, dim=0).to(torch.float32)\n",
    "    \n",
    "    \n",
    "#     return theta_tensor, photon_info_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2574479/1633923016.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(\"snpe_round1_state.pt\",\n",
      "/tmp/ipykernel_2574479/1633923016.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  parameter_range = torch.load('parameter_range_1SFG_1mAGN.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training neural network. Epochs trained: 1"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f22204fff6d45d3ab9fd1af2bbf2d62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating 20 MCMC inits via resample strategy:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "265e846de4854947b207598de7417c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running vectorized MCMC with 20 chains:   0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing iteration 100\n",
      "Processing iteration 60\n",
      "Processing iteration 40\n",
      "Processing iteration 110\n",
      "Processing iteration 90\n",
      "Processing iteration 80\n",
      "Processing iteration 170\n",
      "Processing iteration 20\n",
      "Processing iteration 140\n",
      "Processing iteration 160\n",
      "Processing iteration 70\n",
      "Processing iteration 130\n",
      "Processing iteration 120\n",
      "Processing iteration 10\n",
      "Processing iteration 50\n",
      "Processing iteration 150\n",
      "Processing iteration 190\n",
      "Processing iteration 180\n",
      "Processing iteration 30\n",
      "Processing iteration 200\n",
      "Processing iteration 210\n",
      "Processing iteration 220\n",
      "Processing iteration 230\n",
      "Processing iteration 240\n",
      "Processing iteration 250\n",
      "Processing iteration 260\n",
      "Processing iteration 270\n",
      "Processing iteration 280\n",
      "Processing iteration 290\n",
      "Processing iteration 300\n",
      "Processing iteration 310\n",
      "Processing iteration 320\n",
      "Processing iteration 330\n",
      "Processing iteration 340\n",
      "Processing iteration 350\n",
      "Processing iteration 360\n",
      "Processing iteration 370\n",
      "Processing iteration 380\n",
      "Processing iteration 390\n",
      "Processing iteration 400\n",
      "Processing iteration 410\n",
      "Processing iteration 420\n",
      "Processing iteration 430\n",
      "Processing iteration 440\n",
      "Processing iteration 450\n",
      "Processing iteration 460\n",
      "Processing iteration 470\n",
      "Processing iteration 480\n",
      "Processing iteration 490\n",
      "Processing iteration 500\n",
      "Processing iteration 510\n",
      "Processing iteration 520\n",
      "Processing iteration 530\n",
      "Processing iteration 540\n",
      "Processing iteration 550\n",
      "Processing iteration 560\n",
      "Processing iteration 570\n",
      "Processing iteration 580\n",
      "Processing iteration 590\n",
      "Processing iteration 600\n",
      "Processing iteration 610\n",
      "Processing iteration 620\n",
      "Processing iteration 630\n",
      "Processing iteration 640\n",
      "Processing iteration 650\n",
      "Processing iteration 660\n",
      "Processing iteration 670\n",
      "Processing iteration 680\n",
      "Processing iteration 690\n",
      "Processing iteration 700\n",
      "Processing iteration 710\n",
      "Processing iteration 720\n",
      "Processing iteration 730\n",
      "Processing iteration 740\n",
      "Processing iteration 750\n",
      "Processing iteration 760\n",
      "Processing iteration 770\n",
      "Processing iteration 780\n",
      "Processing iteration 790\n",
      "Processing iteration 800\n",
      "Processing iteration 810\n",
      "Processing iteration 820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/ids29/.conda/envs/sbi_env/lib/python3.13/site-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing iteration 830\n",
      "Processing iteration 840\n",
      "Processing iteration 850\n",
      "Processing iteration 860\n",
      "Processing iteration 870\n",
      "Processing iteration 880\n",
      "Processing iteration 890\n",
      "Processing iteration 900\n",
      "Processing iteration 910\n",
      "Processing iteration 920\n",
      "Processing iteration 930\n",
      "Processing iteration 940\n",
      "Processing iteration 950\n",
      "Processing iteration 960\n",
      "Processing iteration 970\n",
      "Processing iteration 980\n",
      "Processing iteration 990\n",
      "Processing iteration 1000\n"
     ]
    }
   ],
   "source": [
    "class SNPE_C_Custom(SNPE):\n",
    "    def train(self, *a, optimizer_class=None, optimizer_kwargs=None, **kw):\n",
    "        if optimizer_class is None:\n",
    "            return super().train(*a, **kw)\n",
    "        orig = self._build_neural_net\n",
    "        def builder(*aa, **kk):\n",
    "            model = orig(*aa, **kk)\n",
    "            model.optimizer = lambda ps: optimizer_class(ps, **optimizer_kwargs)\n",
    "            return model\n",
    "        self._build_neural_net = builder\n",
    "        try:  return super().train(*a, **kw)\n",
    "        finally: self._build_neural_net = orig\n",
    "\n",
    "# --- 1) load round-1 checkpoint ---------------------------------------\n",
    "ckpt = torch.load(\"snpe_round1_state.pt\",\n",
    "                  map_location=\"cpu\")\n",
    "\n",
    "keep_mask = ckpt[\"keep_mask\"]          # kept summary dims\n",
    "x_mean    = ckpt[\"x_mean\"]              # for later use if needed\n",
    "x_std     = ckpt[\"x_std\"]\n",
    "x_test    = ckpt[\"x_test\"]              # already masked & z-scored\n",
    "\n",
    "\n",
    "parameter_range = torch.load('parameter_range_1SFG_1mAGN.pt')\n",
    "\n",
    "prior = utils.BoxUniform(low=parameter_range[0], high=parameter_range[1])\n",
    "\n",
    "net_builder = posterior_nn(\n",
    "    model=\"nsf\", hidden_features=128, num_transforms=8,\n",
    "    dropout_probability=0.2, use_combined_loss=True,\n",
    "    z_score_x=\"none\", z_score_theta=\"none\",\n",
    ")\n",
    "\n",
    "\n",
    "# fresh SNPE object\n",
    "inf = SNPE_C_Custom(prior, net_builder)\n",
    "\n",
    "# ---------- 3) build network once then load weights -------------------\n",
    "theta_dummy = ckpt[\"theta\"][:2]\n",
    "x_dummy     = ckpt[\"x\"][:2]\n",
    "inf.append_simulations(theta_dummy, x_dummy)\n",
    "_ = inf.train(\n",
    "        training_batch_size = 1,     # ≥ 1\n",
    "        validation_fraction = 0.5,   # anything > 0\n",
    "        max_num_epochs      = 0,     # ← still zero optimisation steps\n",
    "        show_train_summary  = False,\n",
    ")\n",
    "inf._neural_net.load_state_dict(ckpt[\"de_state\"])  # warm-start weights\n",
    "\n",
    "# ---------- 4) posterior conditional on x₀ and draw θ_new -------------\n",
    "posterior = inf.build_posterior(inf._neural_net, sample_with=\"mcmc\")\n",
    "posterior.set_default_x(x_test)                    # x₀\n",
    "\n",
    "num_new   = num_simulations\n",
    "theta_new = posterior.sample((num_new,))           # [num_simulations, 2]\n",
    "\n",
    "# ---------- 5) simulate photons in parallel ---------------------------\n",
    "# def simulate_one(theta_vec):\n",
    "#     return theta_vec, simulator(theta_vec)         # your simulator fn\n",
    "\n",
    "# res = Parallel(n_jobs=num_workers)(\n",
    "#     delayed(simulate_one)(th) for th in theta_new)\n",
    "\n",
    "def simulate_one(i, theta_vec):\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"Processing iteration {i + 1}\")\n",
    "    return theta_vec, simulator(theta_vec)  # your simulator fn\n",
    "\n",
    "res = Parallel(n_jobs=num_workers)(\n",
    "    delayed(simulate_one)(i, th) for i, th in enumerate(theta_new)\n",
    ")\n",
    "\n",
    "theta_new_tensor, photon_info_new = zip(*res)\n",
    "theta_new_tensor = torch.stack(theta_new_tensor)\n",
    "\n",
    "\n",
    "\n",
    "# theta_new_tensor, photon_info_new  = manual_simulate_for_sbi(posterior,\n",
    "#                                    num_simulations=num_simulations,\n",
    "#                                    num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round-2 simulations complete and saved.\n"
     ]
    }
   ],
   "source": [
    "# ---------- 6) save round-2 raw data ----------------------------------\n",
    "torch.save(theta_new_tensor,\n",
    "           \"case1_theta_round2.pt\")\n",
    "with open(\"case1_photon_info_round2.pkl\", \"wb\") as f:\n",
    "    pickle.dump(photon_info_new, f)\n",
    "\n",
    "print(\"Round-2 simulations complete and saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbi_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

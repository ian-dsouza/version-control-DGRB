{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4b18cda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7472a842-c503-4285-9818-7af9e069268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/users/ids29/DGRB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cc88afbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aegis\n",
    "import numpy as np\n",
    "import healpy as hp\n",
    "import torch\n",
    "import pickle as pk\n",
    "from astropy import units as u\n",
    "from astropy import constants as c\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "import os\n",
    "from sbi.inference import SNPE #, prepare_for_sbi #, simulate_for_sbi\n",
    "from sbi import utils as utils\n",
    "from sbi import analysis as analysis\n",
    "# from sbi.inference.base import infer\n",
    "from getdist import plots, MCSamples\n",
    "import pickle\n",
    "from scipy.stats import norm, skew, kurtosis, spearmanr\n",
    "from scipy.integrate import quad, simpson\n",
    "from joblib import Parallel, delayed\n",
    "from torch.optim import AdamW\n",
    "from sbi.neural_nets import posterior_nn\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "87cebaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOW: tensor([-1, -4]); HIGH: tensor([ 2, -1])\n",
      "Completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1356725/3981598522.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  prior_param_range = torch.load('parameter_range_Poisson_SFG1.pt')\n",
      "/tmp/ipykernel_1356725/3981598522.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  theta_test = torch.load('theta_test_Poisson_SFG1_part1.pt')\n"
     ]
    }
   ],
   "source": [
    "grains=1000\n",
    "num_workers = 48  # or your chosen parallel worker count\n",
    "\n",
    "NUM_ROUNDS     = 1\n",
    "SIMS_PER_ROUND = int(1e3)          # Change only if the script that generates the data uses a different number of simulations\n",
    "POST_SAMPLES   = 10_000            # how many samples for corner plot\n",
    "\n",
    "prior_param_range = torch.load('parameter_range_Poisson_SFG1.pt')\n",
    "\n",
    "LOW  =   prior_param_range[0,:]\n",
    "HIGH =   prior_param_range[1,:]\n",
    "print(f\"LOW: {LOW}; HIGH: {HIGH}\")\n",
    "LABELS = [r\"\\log_{10}A_{\\text{Poisson}}\", r\"\\log_{10}\\Phi_{\\rm SFG}\"]\n",
    "\n",
    "theta_test = torch.load('theta_test_Poisson_SFG1_part1.pt')\n",
    "# print(f\"theta_test: {theta_test}\")\n",
    "\n",
    "use_energy_info = False\n",
    "\n",
    "print('Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "32638c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n"
     ]
    }
   ],
   "source": [
    "parameter_range = [[], []]\n",
    "abundance_luminosity_and_spectrum_list = []\n",
    "source_class_list = []\n",
    "parameter_names = []\n",
    "energy_range = [1000, 100000] #MeV\n",
    "energy_range_gen = [energy_range[0]*0.5, energy_range[1]*18]\n",
    "max_radius = 8.5 + 20*2 #kpc\n",
    "exposure = 2000*10*0.2 #cm^2 yr\n",
    "flux_cut = 1e-9 #photons/cm^2/s\n",
    "angular_cut = np.pi #10*u.deg.to('rad') #degrees\n",
    "angular_cut_gen = np.pi #angular_cut*1.5\n",
    "lat_cut = 0 #2*u.deg.to('rad') #degrees\n",
    "lat_cut_gen = lat_cut*0.5\n",
    "\n",
    "\n",
    "print('Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e425ee8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n"
     ]
    }
   ],
   "source": [
    "my_cosmology = 'Planck18'\n",
    "z_range = [0, 14]\n",
    "luminosity_range = 10.0**np.array([37, 50]) # Minimum value set by considering Andromeda distance using Fermi as benchmark and receiving 0.1 photon at detector side\n",
    "my_AEGIS = aegis.aegis(abundance_luminosity_and_spectrum_list, source_class_list, parameter_range, energy_range, luminosity_range, max_radius, exposure, angular_cut, lat_cut, flux_cut, energy_range_gen=energy_range_gen, cosmology = my_cosmology, z_range = z_range, verbose = False)\n",
    "my_AEGIS.angular_cut_gen, my_AEGIS.lat_cut_gen = angular_cut_gen, lat_cut_gen\n",
    "\n",
    "print('Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b51fe0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n"
     ]
    }
   ],
   "source": [
    "def compute_moments(values):\n",
    "    \"\"\"Compute the mean and variance of the energies.\"\"\"\n",
    "    mean = np.mean(values)\n",
    "    var = np.var(values)\n",
    "    skewness = skew(values)\n",
    "    kurt = kurtosis(values)\n",
    "    return mean, var, skewness, kurt\n",
    "\n",
    "def compute_quantiles(values, quantiles=[10, 25, 50, 75, 90]):\n",
    "    \"\"\"\n",
    "    Compute the specified quantiles (in percent).\n",
    "    For example, the 25th quantile is the energy such that 25% of the data lies below it.\n",
    "    Returns a dictionary mapping percentiles to values.\n",
    "    \"\"\"\n",
    "    q_values = np.percentile(values, quantiles)\n",
    "    return dict(zip(quantiles, q_values))\n",
    "\n",
    "def normalize_quantiles(q_dict, val_min, val_max):\n",
    "    \"\"\"\n",
    "    Normalize quantile values from a dictionary using min-max normalization.\n",
    "    \"\"\"\n",
    "    norm_q = {perc: (val - val_min) / (val_max - val_min) for perc, val in q_dict.items()}\n",
    "    return norm_q\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def effective_spectral_index(energies, E_lower):\n",
    "    \"\"\"\n",
    "    Compute effective spectral index via MLE.\n",
    "    Returns np.nan if there are no photons.\n",
    "    \"\"\"\n",
    "    energies = np.array(energies)\n",
    "    n = len(energies)\n",
    "    if n == 0:\n",
    "        return np.nan\n",
    "    sum_logs = np.sum(np.log(energies / E_lower))\n",
    "    return 1 + n / sum_logs\n",
    "\n",
    "def smooth_binned_gamma(binned_gamma, counts, global_gamma, min_count=6):\n",
    "    \"\"\"\n",
    "    Smooth binned effective spectral indices if the count in a bin is low.\n",
    "    \n",
    "    Parameters:\n",
    "      binned_gamma : array-like\n",
    "          Array of spectral indices per energy bin (may contain np.nan for empty bins).\n",
    "      counts : array-like\n",
    "          Array of photon counts per bin.\n",
    "      global_gamma : float\n",
    "          Global effective spectral index, used as fallback.\n",
    "      min_count : int\n",
    "          Minimum photon count required to trust a bin.\n",
    "    \n",
    "    Returns:\n",
    "      smoothed_gamma : numpy array with smoothed spectral index values.\n",
    "    \"\"\"\n",
    "    binned_gamma = np.array(binned_gamma)\n",
    "    counts = np.array(counts)\n",
    "    num_bins = len(binned_gamma)\n",
    "    smoothed_gamma = binned_gamma.copy()\n",
    "\n",
    "    # Loop over each bin.\n",
    "    for i in range(num_bins):\n",
    "        if counts[i] < min_count or np.isnan(binned_gamma[i]):\n",
    "            indices = [i]\n",
    "            if i > 0:\n",
    "                indices.append(i-1)\n",
    "            if i < num_bins - 1:\n",
    "                indices.append(i+1)\n",
    "            indices = list(set(indices))  # ensure uniqueness\n",
    "            \n",
    "            # Only include indices with non-NaN values\n",
    "            valid_indices = [j for j in indices if not np.isnan(binned_gamma[j])]\n",
    "            if valid_indices:\n",
    "                # Use the counts as weights. If any count is zero, it's fine; it will reduce the weight.\n",
    "                weights = np.array([counts[j] for j in valid_indices], dtype=np.float32)\n",
    "                # If all weights sum to 0, default to global_gamma.\n",
    "                if np.sum(weights) > 0:\n",
    "                    smoothed_gamma[i] = np.average(binned_gamma[valid_indices], weights=weights)\n",
    "                else:\n",
    "                    smoothed_gamma[i] = global_gamma\n",
    "            else:\n",
    "                # If none of the neighboring bins have a valid estimate, fallback to global_gamma.\n",
    "                smoothed_gamma[i] = global_gamma\n",
    "    return smoothed_gamma\n",
    "\n",
    "def compute_binned_effective_spectral_indices(energies, num_bins, energy_range):\n",
    "    \"\"\"\n",
    "    Divide the energy range into logarithmic bins and compute the effective spectral index in each bin.\n",
    "    For bins with few photons (count < min_count) or empty bins, apply smoothing by averaging with neighboring bins.\n",
    "    \n",
    "    Returns:\n",
    "      smoothed_gamma : 1D array containing the (possibly smoothed) spectral index for each bin.\n",
    "      global_gamma  : Effective spectral index computed using all energies (with the lowest bin edge as E_lower).\n",
    "      counts        : Raw photon counts per bin.\n",
    "    \"\"\"\n",
    "    bins = np.geomspace(energy_range[0], energy_range[1], num_bins + 1)\n",
    "    binned_gamma = []\n",
    "    counts = []  # photon counts per bin\n",
    "\n",
    "    # Loop over bins\n",
    "    for i in range(len(bins) - 1):\n",
    "        mask = (energies >= bins[i]) & (energies < bins[i+1])\n",
    "        energies_bin = energies[mask]\n",
    "        counts.append(len(energies_bin))\n",
    "        # Compute gamma for the bin; if the bin is empty, effective_spectral_index returns np.nan.\n",
    "        gamma = effective_spectral_index(energies_bin, E_lower=bins[i])\n",
    "        binned_gamma.append(gamma)\n",
    "    \n",
    "    binned_gamma = np.array(binned_gamma)\n",
    "    counts = np.array(counts)\n",
    "    \n",
    "    # Compute the global effective spectral index over all energies using the first bin's lower edge.\n",
    "    global_gamma = effective_spectral_index(energies, E_lower=bins[0])\n",
    "    \n",
    "    # Smooth the binned_gamma using neighboring bins if counts in a bin are low.\n",
    "    smoothed_gamma = smooth_binned_gamma(binned_gamma, counts, global_gamma, min_count=6)\n",
    "    \n",
    "    return smoothed_gamma, global_gamma, counts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def normalize_binned_gamma(binned_gamma, global_gamma, mode='ratio'):\n",
    "    \"\"\"\n",
    "    Normalize the binned effective spectral indices relative to the global effective spectral index.\n",
    "    \n",
    "    Parameters:\n",
    "      binned_gamma : np.array\n",
    "          Array of effective spectral indices for each energy bin.\n",
    "      global_gamma : float\n",
    "          The global effective spectral index computed over the entire energy range.\n",
    "      mode : str, optional\n",
    "          'ratio' for normalized_binned_gamma = binned_gamma / global_gamma,\n",
    "          'difference' for normalized_binned_gamma = (binned_gamma - global_gamma)/global_gamma.\n",
    "          Default is 'ratio'.\n",
    "    \n",
    "    Returns:\n",
    "      normalized_binned_gamma : np.array\n",
    "    \"\"\"\n",
    "    if mode == 'ratio':\n",
    "        # Return ratio relative to global spectral index\n",
    "        normalized_binned_gamma = binned_gamma / global_gamma\n",
    "    elif mode == 'difference':\n",
    "        # Return relative differences with baseline 0\n",
    "        normalized_binned_gamma = (binned_gamma - global_gamma) / global_gamma\n",
    "    else:\n",
    "        raise ValueError(\"Unknown mode for normalization. Choose 'ratio' or 'difference'.\")\n",
    "    return normalized_binned_gamma\n",
    "\n",
    "\n",
    "# --- concise, log-binned 1-D energy histogram ---------------------------\n",
    "def compute_energy_only_histogram(energies, num_bins, energy_range=(1000, 100_000)):\n",
    "    \"\"\"\n",
    "    Log-bin photon energies (MeV) into `num_bins` and return raw counts.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    energies      : 1-D iterable (list / np.ndarray / torch.Tensor)\n",
    "    num_bins      : int, number of logarithmic bins\n",
    "    energy_range  : (low, high) edges in MeV  (default 1–100 GeV)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    hist : 1-D np.ndarray length `num_bins`  (integer counts)\n",
    "    \"\"\"\n",
    "    edges = np.geomspace(energy_range[0], energy_range[1], num_bins + 1, dtype=np.float32)\n",
    "    hist, _ = np.histogram(np.asarray(energies, dtype=np.float32), bins=edges)\n",
    "    return hist.astype(np.float32)\n",
    "\n",
    "\n",
    "def normalize_energy_only_hist(energy_only_hist):\n",
    "    \"\"\"\n",
    "    Normalize a 1D histogram so it sums to 1.\n",
    "    \"\"\"\n",
    "    total = np.sum(energy_only_hist) # this is the total number of photons in the energy-only histogram\n",
    "    if total == 0:\n",
    "        return energy_only_hist  # or handle the zero-case as needed\n",
    "    return energy_only_hist / total\n",
    "\n",
    "def compute_sub_statistics(energies, counts, N_Ebins):\n",
    "    \"\"\"\n",
    "    Given an array of photon energies (all between 1 and 100 GeV), compute a set of summary statistics:\n",
    "      1. Mean energy.\n",
    "      2. Variance of energy.\n",
    "      3. Quantiles: 10%, 25%, 50%, 75%, and 90%.\n",
    "      4. Effective spectral index estimated from the data.\n",
    "      \n",
    "    Returns the statistics in a dictionary and also a flattened torch tensor.\n",
    "    \"\"\"\n",
    "    # 1. Mean and variance\n",
    "    mean_E, var_E, skewness_E, kurt_E  = compute_moments(energies)\n",
    "    mean_counts, var_counts, skewness_counts, kurt_counts  = compute_moments(counts)\n",
    "    \n",
    "    # 2. Quantiles\n",
    "    quant_dict_E = compute_quantiles(energies)  # This returns a dict like {10: val, 25: val, ...}\n",
    "    norm_quant_dict_E = normalize_quantiles(quant_dict_E, val_min=energy_range[0], val_max=energy_range[1])\n",
    "\n",
    "    quant_dict_counts = compute_quantiles(counts)  # This returns a dict like {10: val, 25: val, ...}\n",
    "    norm_quant_dict_counts = normalize_quantiles(quant_dict_counts, val_min=0, val_max=np.max(counts))\n",
    "\n",
    "    energy_only_hist = compute_energy_only_histogram(energies, num_bins=N_Ebins, energy_range=energy_range)\n",
    "\n",
    "    binned_gamma, global_gamma, energy_only_hist = compute_binned_effective_spectral_indices(energies, num_bins=N_Ebins, energy_range=energy_range)\n",
    "    norm_binned_gamma = normalize_binned_gamma(binned_gamma, global_gamma, mode='ratio')\n",
    "    norm_energy_only_hist = normalize_energy_only_hist(energy_only_hist)\n",
    "    \n",
    "    # If you want to pass the summary statistic to sbi, it is best to use a fixed-size vector (e.g., a torch tensor).\n",
    "    # For example, arrange the stats in a consistent order:\n",
    "    energy_only_scalars  = np.array([\n",
    "        mean_E, var_E , skewness_E, kurt_E,\n",
    "        global_gamma\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    other_scalars  = np.array([\n",
    "        np.log10(energies.size), # total number of photons\n",
    "        mean_counts, var_counts, skewness_counts, kurt_counts,\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    \n",
    "    return energy_only_scalars, other_scalars, norm_quant_dict_E, norm_quant_dict_counts, norm_binned_gamma, norm_energy_only_hist\n",
    "\n",
    "def normalize_energy_dependent_hist(energy_dependent_hist, total_pixels):\n",
    "    \"\"\"\n",
    "    Normalize a 2D histogram so that each column sums to 1.\n",
    "    total_pixels: the fixed number of pixels (for example, len(roi_pix_i))\n",
    "    \"\"\"\n",
    "    # If the histogram is of shape (N_countbins, N_Ebins),\n",
    "    # each column should ideally sum to total_pixels.\n",
    "    normalized_hist = energy_dependent_hist / total_pixels\n",
    "    return normalized_hist\n",
    "\n",
    "def fano_from_photons(photon_info, nsides=(32, 64, 128), center_mask_deg=10, lat_mask_deg=5): #Outputs an array of Fano factors for each NSIDE (currently three in number).\n",
    "    \"\"\"\n",
    "    photon_info must expose `theta` (co-latitude, rad) and `phi` (longitude, rad).\n",
    "    Returns ΔF for each NSIDE in `nsides`.\n",
    "    \"\"\"\n",
    "    # theta_p = photon_info['theta']   # or however your struct stores it\n",
    "    # phi_p   = photon_info['phi']\n",
    "\n",
    "    deltaF = []\n",
    "    for N_side in nsides:\n",
    "\n",
    "        N_pix = 12*N_side**2\n",
    "        pix_i = np.linspace(0, N_pix-1, N_pix, dtype = 'int')\n",
    "        roi_pix_i = np.where(np.logical_and(hp.rotator.angdist(np.array([np.pi/2, 0]), hp.pix2ang(N_side, pix_i)) >= center_mask_deg*u.deg.to('rad'), np.abs(np.pi/2 - hp.pix2ang(N_side, pix_i)[0]) >= lat_mask_deg*u.deg.to('rad')))[0]\n",
    "\n",
    "        N_Ebins = 1 # This value doesn't matter because in 'get_roi_map_summary' fucntion, when you use 'Ebinspace = single', it will only consider one energy bin, and not use the N_Ebins value.\n",
    "        roi_map_1D = my_AEGIS.get_roi_map_summary(photon_info = photon_info, N_side = N_side, N_Ebins = N_Ebins, Ebinspace = 'single', roi_pix_i = roi_pix_i) # 'single' means only one energy bin.\n",
    "        counts = roi_map_1D.ravel()\n",
    "\n",
    "        #Fano excess\n",
    "        mu  = counts.mean()\n",
    "        var = counts.var()\n",
    "        deltaF.append(max(0.0, var/mu - 1.0) if mu > 0 else 0.0)\n",
    "\n",
    "    return np.asarray(deltaF, dtype=np.float32)\n",
    "\n",
    "\n",
    "\n",
    "print('Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "110b4ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n"
     ]
    }
   ],
   "source": [
    "# a simple simulator with the total number of photons as the summary statistic\n",
    "def compute_summary_statistics(photon_info):\n",
    "\n",
    "    N_side = 2**6\n",
    "    #parameters for the summary statistic\n",
    "    \n",
    "    center_mask = 10 #deg \n",
    "    lat_mask = 5 #deg \n",
    "    N_Ebins = 20\n",
    "    # Ebinspace = 'log'#'linear'\n",
    "    N_countbins = 10\n",
    "    countbinspace = 'custom'#'linear'\n",
    "    mincount_2D_hist, maxcount_2D_hist = 0, 38 # For energy-dependent 2D histogram # Change for different data sets. Consider the data set with the maximum number of photons to decide the value of 'maxcount'.\n",
    "    mincount_1D_counts_hist, maxcount_1D_counts_hist = 0, 123 # For counts only 1D histogram # Change for different data sets. Consider the data set with the maximum number of photons to decide the value of 'maxcount'.\n",
    "    N_pix = 12*N_side**2\n",
    "    pix_i = np.linspace(0, N_pix-1, N_pix, dtype = 'int')\n",
    "    roi_pix_i = np.where(np.logical_and(hp.rotator.angdist(np.array([np.pi/2, 0]), hp.pix2ang(N_side, pix_i)) >= center_mask*u.deg.to('rad'), np.abs(np.pi/2 - hp.pix2ang(N_side, pix_i)[0]) >= lat_mask*u.deg.to('rad')))[0]\n",
    "\n",
    "    # Get energy dependent 2D histogram\n",
    "    roi_map_2D = my_AEGIS.get_roi_map_summary(photon_info = photon_info, N_side = N_side, N_Ebins = N_Ebins, Ebinspace = 'log', roi_pix_i = roi_pix_i)\n",
    "    # print(f\"For energy-dependent hist, max counts = {np.max(roi_map)}; roi_map.shape = {roi_map.shape}\")\n",
    "    energy_dependent_hist = my_AEGIS.get_counts_histogram_from_roi_map(roi_map_2D, mincount = mincount_2D_hist, maxcount = maxcount_2D_hist, N_countbins = N_countbins, countbinspace = countbinspace)\n",
    "    total_pixels = roi_pix_i.size\n",
    "    norm_energy_dependent_hist = normalize_energy_dependent_hist(energy_dependent_hist, total_pixels)\n",
    "\n",
    "    # Get counts-only 1D histogram\n",
    "    roi_map_1D = my_AEGIS.get_roi_map_summary(photon_info = photon_info, N_side = N_side, N_Ebins = N_Ebins, Ebinspace = 'single', roi_pix_i = roi_pix_i) # 'single' means only one energy bin.\n",
    "    # print(f\"For counts-only hist, max counts = {np.max(roi_map)}; roi_map.shape = {roi_map.shape}\")\n",
    "    counts_only_hist = my_AEGIS.get_counts_histogram_from_roi_map(roi_map_1D, mincount = mincount_1D_counts_hist, maxcount = maxcount_1D_counts_hist, N_countbins = N_countbins, countbinspace = countbinspace)\n",
    "    total_pixels = roi_pix_i.size\n",
    "    norm_counts_only_hist = normalize_energy_dependent_hist(counts_only_hist, total_pixels)\n",
    "\n",
    "\n",
    "    photon_pixels = hp.ang2pix(N_side, photon_info['angles'][:, 0], photon_info['angles'][:, 1])\n",
    "    roi_mask = np.isin(photon_pixels, roi_pix_i)\n",
    "    energies_in_roi = photon_info['energies'][roi_mask]\n",
    "\n",
    "    counts = roi_map_1D.ravel()\n",
    "    energy_only_scalars, other_scalars, norm_quant_dict_E, norm_quant_dict_counts, norm_binned_gamma, norm_energy_only_hist = compute_sub_statistics(energies_in_roi, counts, N_Ebins = N_Ebins) # Original code\n",
    "    \n",
    "    QUANTILES = [10, 25, 50, 75, 90]\n",
    "    sorted_keys_E = sorted(norm_quant_dict_E.keys())\n",
    "    assert sorted_keys_E == QUANTILES, \"Quantile keys differ from expected set\"\n",
    "    norm_quant_vector_E = np.array([norm_quant_dict_E[k] for k in sorted_keys_E], dtype=np.float32)\n",
    "\n",
    "    sorted_keys_counts = sorted(norm_quant_dict_counts.keys())\n",
    "    assert sorted_keys_counts == QUANTILES, \"Quantile keys differ from expected set\"\n",
    "    norm_quant_vector_counts = np.array([norm_quant_dict_counts[k] for k in sorted_keys_counts], dtype=np.float32)\n",
    "\n",
    "    flat_norm_energy_dependent_hist = np.asarray(norm_energy_dependent_hist, dtype=np.float32).flatten()\n",
    "    flat_norm_counts_only_hist = np.asarray(norm_counts_only_hist, dtype=np.float32).flatten()\n",
    "\n",
    "    fano_factors = fano_from_photons(photon_info, center_mask_deg=center_mask, lat_mask_deg=lat_mask) # evaluated at N-sides 32, 64, and 128\n",
    "\n",
    "    # if use_energy_info:\n",
    "    #     ## This is the full summary statistic array\n",
    "    #     parts = [          \n",
    "    #         energy_only_scalars,                                         # scalars that depend only on energy\n",
    "    #         other_scalars,                                              # scalars that don't depend on energy\n",
    "    #         np.array([np.max(counts)], dtype=np.float32), # max counts that any pixel sees\n",
    "    #         fano_factors,\n",
    "    #         norm_quant_vector_E,\n",
    "    #         norm_quant_vector_counts,\n",
    "    #         norm_binned_gamma,\n",
    "    #         norm_energy_only_hist,\n",
    "    #         flat_norm_counts_only_hist,\n",
    "    #         flat_norm_energy_dependent_hist\n",
    "    #     ]\n",
    "    # else:\n",
    "    #     ## This is the reduced summary statistic array, which excludes energy-only information\n",
    "    #     parts = [          \n",
    "    #         other_scalars,                                          # all features except the scalars\n",
    "    #         np.array([np.max(counts)], dtype=np.float32), # max counts that any pixel sees\n",
    "    #         fano_factors,\n",
    "    #         norm_quant_vector_counts,\n",
    "    #         flat_norm_counts_only_hist,\n",
    "    #         flat_norm_energy_dependent_hist\n",
    "    #     ]\n",
    "    \n",
    "\n",
    "    # summary_array = np.concatenate(parts, axis=0)\n",
    "    \n",
    "    # return torch.as_tensor(summary_array, dtype=torch.float32)   # stays on CPU\n",
    "\n",
    "    # TRYING TO REDUCE THE NUMBER OF SUMMARIES\n",
    "    parts = [\n",
    "        np.array([other_scalars[0]])\n",
    "    ]\n",
    "\n",
    "    summary_array = np.concatenate(parts, axis=0)\n",
    "    \n",
    "    return torch.as_tensor(summary_array, dtype=torch.float32)   # stays on CPU\n",
    "\n",
    "print('Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b7fd7041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# helper: remove zero-var + low-ρ columns\n",
    "# ----------------------------------------------------------------------\n",
    "def reduce_summary(x_train_full: torch.Tensor,\n",
    "                   x_test_full:  torch.Tensor,\n",
    "                   theta_train:  torch.Tensor,\n",
    "                   thresh: float = 0.10):\n",
    "    \"\"\"\n",
    "    Drop (a) zero-variance columns and (b) columns with |Spearman ρ| ≤ thresh\n",
    "    for every parameter.  Works for either a single test vector (1-D) or a\n",
    "    batch of test summaries (2-D).\n",
    "    \"\"\"\n",
    "    # ---------- 1) zero-variance ------------------------------------------\n",
    "    stds   = x_train_full.std(0)\n",
    "    keep_z = stds > 1e-8\n",
    "\n",
    "    # work on the already trimmed train set\n",
    "    x_tr_z = x_train_full[:, keep_z]\n",
    "\n",
    "    # ---------- 2) Spearman ρ ---------------------------------------------\n",
    "    x_np, θ_np = x_tr_z.numpy(), theta_train.numpy()\n",
    "    D_red, P   = x_np.shape[1], θ_np.shape[1]\n",
    "    rho = np.empty((D_red, P))\n",
    "    for i in range(D_red):\n",
    "        for j in range(P):\n",
    "            r, _ = spearmanr(x_np[:, i], θ_np[:, j])\n",
    "            rho[i, j] = 0.0 if np.isnan(r) else r\n",
    "    keep_r = (np.abs(rho) > thresh).any(axis=1)   # informative columns\n",
    "\n",
    "    # ---------- 3) final mask in the ORIGINAL D ---------------------------\n",
    "    keep = torch.zeros_like(keep_z)\n",
    "    keep[keep_z] = torch.from_numpy(keep_r)       # apply both filters\n",
    "\n",
    "    # ---------- 4) slice both train & test --------------------------------\n",
    "    x_train_reduced = x_train_full[:, keep]\n",
    "\n",
    "    if x_test_full.ndim == 1:          # single test vector\n",
    "        x_test_reduced = x_test_full[keep]\n",
    "    else:                              # batch of test summaries\n",
    "        x_test_reduced = x_test_full[:, keep]\n",
    "\n",
    "    return x_train_reduced, x_test_reduced, keep\n",
    "\n",
    "\n",
    "class SNPE_C_Custom(SNPE):\n",
    "    def train(self, *a, optimizer_class=None, optimizer_kwargs=None, **kw):\n",
    "        if optimizer_class is None:\n",
    "            return super().train(*a, **kw)\n",
    "        orig = self._build_neural_net\n",
    "        def builder(*aa, **kk):\n",
    "            model = orig(*aa, **kk)\n",
    "            model.optimizer = lambda ps: optimizer_class(ps, **optimizer_kwargs)\n",
    "            return model\n",
    "        self._build_neural_net = builder\n",
    "        try:  return super().train(*a, **kw)\n",
    "        finally: self._build_neural_net = orig\n",
    "\n",
    "print('Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a2e11c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ------------------------------------------------------------------ #\n",
    "# # 5.  Initialise\n",
    "# # ------------------------------------------------------------------ #\n",
    "# prior     = utils.BoxUniform(LOW, HIGH, device=\"cpu\")   # cpu for sampling speed\n",
    "# with open('test_photon_info_Poisson_SFG1_part1.pkl', 'rb') as f:\n",
    "#     raw_o = pickle.load(f) # raw photon data for test case\n",
    "# # raw_o =  simulate_raw_photon_data(theta_test)  # simulate the observed raw data for test case\n",
    "# x_o_full  = compute_summary_statistics(raw_o)       # 1-D tensor  (D_full,)\n",
    "\n",
    "\n",
    "# θ_batch = torch.load('thetas_training_Poisson_SFG1.pt') # thetas used for training dataset\n",
    "# with open('training_photon_info_Poisson_SFG1.pkl', 'rb') as f:\n",
    "#     raw_training = pickle.load(f) # raw photon data for training dataset\n",
    "\n",
    "\n",
    "# mean_x, std_x = None, None          # will freeze after round-1\n",
    "# mask_keep      = None               # pruning mask  (booleans over D_full)\n",
    "\n",
    "# Θ_accum, X_accum = [], []           # running simulation database\n",
    "# proposal = prior                    # start broad\n",
    "# plt.ion()\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # resume_flag = False   # Nothing to resume yet\n",
    "\n",
    "# for r in range(1, NUM_ROUNDS + 1):\n",
    "#     print(f\"\\n──────────  ROUND {r}/{NUM_ROUNDS}  ──────────\")\n",
    "\n",
    "#     # 5.1  Draw parameters\n",
    "#     # if r == 1:\n",
    "#     #     θ_batch = prior.sample((SIMS_PER_ROUND,))    # (N,2) cpu\n",
    "#     # else:\n",
    "#     #     θ_batch = proposal.sample((SIMS_PER_ROUND,), x=x_o_z).cpu()\n",
    "\n",
    "#     # 5.2  Simulate & summarise\n",
    "#     # summaries = []\n",
    "#     # for θ in θ_batch:\n",
    "#     #     raw   = simulate_raw_photon_data(θ)\n",
    "#     #     summ  = compute_summary_statistics(raw)\n",
    "#     #     summaries.append(summ)\n",
    "#     # x_batch_full = torch.stack(summaries)            # (N, D_full)\n",
    "    \n",
    "#     # def simulate_and_summarize(theta):\n",
    "#     #     raw  = simulate_raw_photon_data(theta)\n",
    "#     #     return compute_summary_statistics(raw)\n",
    "\n",
    "#     # dispatch 48 parallel jobs\n",
    "#     summaries = Parallel(\n",
    "#         n_jobs=num_workers,           # number of worker processes\n",
    "#         backend=\"loky\",      # default multiprocess backend\n",
    "#         verbose=10           # progress output\n",
    "#     )(\n",
    "#         delayed(compute_summary_statistics)(raw) \n",
    "#         for raw in raw_training\n",
    "#     )\n",
    "\n",
    "#     # stack back into a tensor\n",
    "#     x_batch_full = torch.stack(summaries)  # shape: (SIMS_PER_ROUND, D_full)\n",
    "\n",
    "#     # 5.3  Append to global store\n",
    "#     Θ_accum.append(θ_batch)\n",
    "#     X_accum.append(x_batch_full)\n",
    "#     Θ_all = torch.cat(Θ_accum, 0)                    # (N_tot,2)\n",
    "#     X_all_full = torch.cat(X_accum, 0)               # (N_tot,D_full)\n",
    "\n",
    "#     # 5.4  Dimension pruning  (mask fixed after first round)\n",
    "#     if mask_keep is None:\n",
    "#         X_red, x_o_red, mask_keep = reduce_summary(\n",
    "#             X_all_full, x_o_full, Θ_all, thresh=0.10)\n",
    "#         print(f\"summary dims: {X_all_full.shape[1]}  →  {X_red.shape[1]} kept\")\n",
    "#     else:\n",
    "#         X_red = X_all_full[:, mask_keep]\n",
    "#         x_o_red = x_o_full[mask_keep]\n",
    "\n",
    "#     # 5.5  External z-score  (μ,σ fixed after first round)\n",
    "#     if mean_x is None:\n",
    "#         mean_x = X_red.mean(0)\n",
    "#         std_x  = X_red.std(0)#.clamp(min=1e-8)\n",
    "#     X_z = (X_red - mean_x) / std_x\n",
    "#     x_o_z = (x_o_red - mean_x) / std_x\n",
    "\n",
    "#     # 5.6  Build / update inference object\n",
    "#     if r == 1:\n",
    "#         net = posterior_nn(model=\"nsf\", hidden_features=128, num_transforms=8,\n",
    "#                            dropout_probability=0.2, use_combined_loss=True,\n",
    "#                            z_score_x=\"none\", z_score_theta=\"none\")\n",
    "#         # net = posterior_nn(model=\"nsf\", hidden_features=256, num_transforms=12,\n",
    "#         #                    dropout_probability=0.2, use_combined_loss=True,\n",
    "#         #                    z_score_x=\"independent\", z_score_theta=\"none\")\n",
    "#         inf = SNPE_C_Custom(prior, net, device=device)\n",
    "        \n",
    "\n",
    "#     # --- append **only this round’s** simulations and tag them with their proposal ---\n",
    "#     idx0        = Θ_all.shape[0] - SIMS_PER_ROUND      # first row of the current round\n",
    "#     X_z_batch   = X_z[idx0:]                           # shape: (SIMS_PER_ROUND, D_red)\n",
    "\n",
    "#     # proposal must be passed from round 2 onward (None is fine in round 1)\n",
    "#     this_proposal = None if r == 1 else proposal\n",
    "\n",
    "#     inf.append_simulations(\n",
    "#             θ_batch.to(device),        # θ from this round only\n",
    "#             X_z_batch.to(device),      # summaries from this round only\n",
    "#             proposal=this_proposal,\n",
    "#     )\n",
    "\n",
    "\n",
    "#     # inf.append_simulations(Θ_all.to(device), X_z.to(device))\n",
    "\n",
    "\n",
    "\n",
    "#     de = inf.train(\n",
    "#         training_batch_size=800, learning_rate=1e-3,\n",
    "#         validation_fraction=0.2, stop_after_epochs=20, max_num_epochs=500,\n",
    "#         show_train_summary=False, clip_max_norm=1.0,\n",
    "#         optimizer_class=AdamW,\n",
    "#         optimizer_kwargs={\"lr\": 1e-3, \"weight_decay\": 5e-5})\n",
    "    \n",
    "#     # de = inf.train(\n",
    "#     # training_batch_size=256, learning_rate=1e-3,\n",
    "#     # validation_fraction=0.2, stop_after_epochs=50, max_num_epochs=500,\n",
    "#     # show_train_summary=False, clip_max_norm=1.0,\n",
    "#     # optimizer_class=AdamW,\n",
    "#     # optimizer_kwargs={\"lr\": 1e-3, \"weight_decay\": 5e-5})\n",
    "    \n",
    "\n",
    "\n",
    "#     # 5.7  Loss curves\n",
    "#     hist = inf._summary\n",
    "#     fig_loss = plt.figure(figsize=(4,3))\n",
    "#     plt.plot(hist[\"training_loss\"],   label=\"Train\")\n",
    "#     plt.plot(hist[\"validation_loss\"], label=\"Val\")\n",
    "#     plt.xlabel(\"epoch\"); plt.ylabel(\"−log prob\"); plt.legend()\n",
    "#     plt.title(f\"Loss – round {r}\")\n",
    "#     plt.show()\n",
    "\n",
    "#     # 5.8  Posterior & sampling\n",
    "#     posterior = inf.build_posterior(de, sample_with=\"mcmc\")\n",
    "#     posterior.set_default_x(x_o_z.to(device))\n",
    "#     samples_np = posterior.sample((POST_SAMPLES,), x=x_o_z.to(device)\n",
    "#                      ).cpu().numpy()\n",
    "\n",
    "#     # 5.9  Triangle plot\n",
    "#     mcs = MCSamples(samples=samples_np, names=[\"Phi_SFG\",\"Phi_mAGN\"],\n",
    "#                     labels=LABELS, ranges=list(zip(LOW.numpy(), HIGH.numpy())))\n",
    "#     fig_corner = plt.figure()\n",
    "#     gp = plots.get_subplot_plotter()\n",
    "#     gp.settings.title_limit_fontsize = 16 # reference size for 3.5 inch subplot\n",
    "#     gp.settings.axes_fontsize=18\n",
    "#     gp.settings.legend_fontsize = 26\n",
    "#     gp.settings.axes_labelsize = 22\n",
    "#     gp.settings.norm_1d_density = True\n",
    "#     gp.settings.title_limit = 1\n",
    "#     gp.triangle_plot(mcs, filled=True,\n",
    "#                      markers=np.array(theta_test).flatten())\n",
    "#     fig_corner.suptitle(f\"Posterior – round {r}\", y=0.98)\n",
    "#     plt.show()\n",
    "\n",
    "#     # 5.10  Make posterior the next proposal\n",
    "#     proposal = posterior\n",
    "\n",
    "# print(\"\\nAll rounds finished.  Close the figures to exit.\")\n",
    "# plt.ioff()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bfad1b",
   "metadata": {},
   "source": [
    "Upgrading from minimal training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fd66399a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1356725/1870320442.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  θ_batch = torch.load('thetas_training_Poisson_SFG1.pt') # thetas used for training dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "──────────  ROUND 1/1  ──────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=48)]: Using backend LokyBackend with 48 concurrent workers.\n",
      "[Parallel(n_jobs=48)]: Done   2 tasks      | elapsed:   15.4s\n",
      "[Parallel(n_jobs=48)]: Done  17 tasks      | elapsed:   20.5s\n",
      "[Parallel(n_jobs=48)]: Done  32 tasks      | elapsed:   25.9s\n",
      "[Parallel(n_jobs=48)]: Done  49 tasks      | elapsed:   35.3s\n",
      "[Parallel(n_jobs=48)]: Done  66 tasks      | elapsed:   38.6s\n",
      "[Parallel(n_jobs=48)]: Done  85 tasks      | elapsed:   40.6s\n",
      "[Parallel(n_jobs=48)]: Done 104 tasks      | elapsed:   52.9s\n",
      "[Parallel(n_jobs=48)]: Done 125 tasks      | elapsed:   56.0s\n",
      "[Parallel(n_jobs=48)]: Done 146 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=48)]: Done 169 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=48)]: Done 192 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=48)]: Done 217 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=48)]: Done 242 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=48)]: Done 269 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=48)]: Done 296 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=48)]: Done 325 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=48)]: Done 354 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=48)]: Done 385 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=48)]: Done 416 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=48)]: Done 449 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=48)]: Done 482 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=48)]: Done 517 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=48)]: Done 552 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=48)]: Done 589 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=48)]: Done 626 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=48)]: Done 665 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=48)]: Done 704 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=48)]: Done 745 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=48)]: Done 786 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=48)]: Done 829 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=48)]: Done 872 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=48)]: Done 1000 out of 1000 | elapsed:  7.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training neural network. Epochs trained: 183"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-12:\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/home/users/ids29/.conda/envs/sbi_env/lib/python3.13/threading.py\"\u001b[0m, line \u001b[35m1041\u001b[0m, in \u001b[35m_bootstrap_inner\u001b[0m\n",
      "    \u001b[31mself.run\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/users/ids29/.conda/envs/sbi_env/lib/python3.13/site-packages/tensorboard/summary/writer/event_file_writer.py\"\u001b[0m, line \u001b[35m244\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "    \u001b[31mself._run\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/users/ids29/.conda/envs/sbi_env/lib/python3.13/site-packages/tensorboard/summary/writer/event_file_writer.py\"\u001b[0m, line \u001b[35m275\u001b[0m, in \u001b[35m_run\u001b[0m\n",
      "    \u001b[31mself._record_writer.write\u001b[0m\u001b[1;31m(data)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/users/ids29/.conda/envs/sbi_env/lib/python3.13/site-packages/tensorboard/summary/writer/record_writer.py\"\u001b[0m, line \u001b[35m40\u001b[0m, in \u001b[35mwrite\u001b[0m\n",
      "    \u001b[31mself._writer.write\u001b[0m\u001b[1;31m(header + header_crc + data + footer_crc)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/users/ids29/.conda/envs/sbi_env/lib/python3.13/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py\"\u001b[0m, line \u001b[35m775\u001b[0m, in \u001b[35mwrite\u001b[0m\n",
      "    \u001b[31mself.fs.append\u001b[0m\u001b[1;31m(self.filename, file_content, self.binary_mode)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/users/ids29/.conda/envs/sbi_env/lib/python3.13/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py\"\u001b[0m, line \u001b[35m167\u001b[0m, in \u001b[35mappend\u001b[0m\n",
      "    \u001b[31mself._write\u001b[0m\u001b[1;31m(filename, file_content, \"ab\" if binary_mode else \"a\")\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/users/ids29/.conda/envs/sbi_env/lib/python3.13/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py\"\u001b[0m, line \u001b[35m171\u001b[0m, in \u001b[35m_write\u001b[0m\n",
      "    with \u001b[31mio.open\u001b[0m\u001b[1;31m(filename, mode, encoding=encoding)\u001b[0m as f:\n",
      "         \u001b[31m~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "\u001b[1;35mFileNotFoundError\u001b[0m: \u001b[35m[Errno 2] No such file or directory: b'/home/users/ids29/DGRB Scripts/Poisson Model/sbi-logs/NPE_C/2025-05-29T18_51_37.825037/events.out.tfevents.1748501497.kerr.1356725.2'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Neural network successfully converged after 184 epochs."
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: b'/home/users/ids29/DGRB Scripts/Poisson Model/sbi-logs/NPE_C/2025-05-29T18_51_37.825037/events.out.tfevents.1748501497.kerr.1356725.2'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 93\u001b[39m\n\u001b[32m     85\u001b[39m inf.append_simulations(\n\u001b[32m     86\u001b[39m         θ_batch.to(device),        \u001b[38;5;66;03m# θ from this round only\u001b[39;00m\n\u001b[32m     87\u001b[39m         X_z.to(device),      \u001b[38;5;66;03m# summaries from this round only\u001b[39;00m\n\u001b[32m     88\u001b[39m )\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# inf.append_simulations(Θ_all.to(device), X_z.to(device))\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m de = inf.train()\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# de = inf.train(\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[38;5;66;03m#     training_batch_size=256, learning_rate=1e-3,\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m#     validation_fraction=0.2, stop_after_epochs=50, max_num_epochs=500,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \n\u001b[32m    104\u001b[39m \u001b[38;5;66;03m# 5.7  Loss curves\u001b[39;00m\n\u001b[32m    105\u001b[39m hist = inf._summary\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sbi/sbi/inference/trainers/npe/npe_c.py:194\u001b[39m, in \u001b[36mNPE_C.train\u001b[39m\u001b[34m(self, num_atoms, training_batch_size, learning_rate, validation_fraction, stop_after_epochs, max_num_epochs, clip_max_norm, calibration_kernel, resume_training, force_first_round_loss, discard_prior_samples, use_combined_loss, retrain_from_scratch, show_train_summary, dataloader_kwargs)\u001b[39m\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_non_atomic_loss:\n\u001b[32m    191\u001b[39m         \u001b[38;5;66;03m# Take care of z-scoring, pre-compute and store prior terms.\u001b[39;00m\n\u001b[32m    192\u001b[39m         \u001b[38;5;28mself\u001b[39m._set_state_for_mog_proposal()\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().train(**kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sbi/sbi/inference/trainers/npe/npe_base.py:427\u001b[39m, in \u001b[36mPosteriorEstimator.train\u001b[39m\u001b[34m(self, training_batch_size, learning_rate, validation_fraction, stop_after_epochs, max_num_epochs, clip_max_norm, calibration_kernel, resume_training, force_first_round_loss, discard_prior_samples, retrain_from_scratch, show_train_summary, dataloader_kwargs)\u001b[39m\n\u001b[32m    424\u001b[39m \u001b[38;5;28mself\u001b[39m._summary[\u001b[33m\"\u001b[39m\u001b[33mbest_validation_loss\u001b[39m\u001b[33m\"\u001b[39m].append(\u001b[38;5;28mself\u001b[39m._best_val_loss)\n\u001b[32m    426\u001b[39m \u001b[38;5;66;03m# Update tensorboard and summary dict.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m427\u001b[39m \u001b[38;5;28mself\u001b[39m._summarize(round_=\u001b[38;5;28mself\u001b[39m._round)\n\u001b[32m    429\u001b[39m \u001b[38;5;66;03m# Update description for progress bar.\u001b[39;00m\n\u001b[32m    430\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m show_train_summary:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sbi/sbi/inference/trainers/base.py:504\u001b[39m, in \u001b[36mNeuralInference._summarize\u001b[39m\u001b[34m(self, round_)\u001b[39m\n\u001b[32m    498\u001b[39m offset = (\n\u001b[32m    499\u001b[39m     torch.tensor(\u001b[38;5;28mself\u001b[39m._summary[\u001b[33m\"\u001b[39m\u001b[33mepochs_trained\u001b[39m\u001b[33m\"\u001b[39m][:-\u001b[32m1\u001b[39m], dtype=torch.int)\n\u001b[32m    500\u001b[39m     .sum()\n\u001b[32m    501\u001b[39m     .item()\n\u001b[32m    502\u001b[39m )\n\u001b[32m    503\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, vlp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m._summary[\u001b[33m\"\u001b[39m\u001b[33mvalidation_loss\u001b[39m\u001b[33m\"\u001b[39m][offset:]):\n\u001b[32m--> \u001b[39m\u001b[32m504\u001b[39m     \u001b[38;5;28mself\u001b[39m._summary_writer.add_scalar(\n\u001b[32m    505\u001b[39m         tag=\u001b[33m\"\u001b[39m\u001b[33mvalidation_loss\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    506\u001b[39m         scalar_value=vlp,\n\u001b[32m    507\u001b[39m         global_step=offset + i,\n\u001b[32m    508\u001b[39m     )\n\u001b[32m    510\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, tlp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m._summary[\u001b[33m\"\u001b[39m\u001b[33mtraining_loss\u001b[39m\u001b[33m\"\u001b[39m][offset:]):\n\u001b[32m    511\u001b[39m     \u001b[38;5;28mself\u001b[39m._summary_writer.add_scalar(\n\u001b[32m    512\u001b[39m         tag=\u001b[33m\"\u001b[39m\u001b[33mtraining_loss\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    513\u001b[39m         scalar_value=tlp,\n\u001b[32m    514\u001b[39m         global_step=offset + i,\n\u001b[32m    515\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/sbi_env/lib/python3.13/site-packages/torch/utils/tensorboard/writer.py:381\u001b[39m, in \u001b[36mSummaryWriter.add_scalar\u001b[39m\u001b[34m(self, tag, scalar_value, global_step, walltime, new_style, double_precision)\u001b[39m\n\u001b[32m    376\u001b[39m torch._C._log_api_usage_once(\u001b[33m\"\u001b[39m\u001b[33mtensorboard.logging.add_scalar\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    378\u001b[39m summary = scalar(\n\u001b[32m    379\u001b[39m     tag, scalar_value, new_style=new_style, double_precision=double_precision\n\u001b[32m    380\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m \u001b[38;5;28mself\u001b[39m._get_file_writer().add_summary(summary, global_step, walltime)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/sbi_env/lib/python3.13/site-packages/torch/utils/tensorboard/writer.py:115\u001b[39m, in \u001b[36mFileWriter.add_summary\u001b[39m\u001b[34m(self, summary, global_step, walltime)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Add a `Summary` protocol buffer to the event file.\u001b[39;00m\n\u001b[32m    103\u001b[39m \n\u001b[32m    104\u001b[39m \u001b[33;03mThis method wraps the provided summary in an `Event` protocol buffer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    112\u001b[39m \u001b[33;03m    walltime (from time.time()) seconds after epoch\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    114\u001b[39m event = event_pb2.Event(summary=summary)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m \u001b[38;5;28mself\u001b[39m.add_event(event, global_step, walltime)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/sbi_env/lib/python3.13/site-packages/torch/utils/tensorboard/writer.py:99\u001b[39m, in \u001b[36mFileWriter.add_event\u001b[39m\u001b[34m(self, event, step, walltime)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     96\u001b[39m     \u001b[38;5;66;03m# Make sure step is converted from numpy or other formats\u001b[39;00m\n\u001b[32m     97\u001b[39m     \u001b[38;5;66;03m# since protobuf might not convert depending on version\u001b[39;00m\n\u001b[32m     98\u001b[39m     event.step = \u001b[38;5;28mint\u001b[39m(step)\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m \u001b[38;5;28mself\u001b[39m.event_writer.add_event(event)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/sbi_env/lib/python3.13/site-packages/tensorboard/summary/writer/event_file_writer.py:117\u001b[39m, in \u001b[36mEventFileWriter.add_event\u001b[39m\u001b[34m(self, event)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, event_pb2.Event):\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    114\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mExpected an event_pb2.Event proto, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    115\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % \u001b[38;5;28mtype\u001b[39m(event)\n\u001b[32m    116\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28mself\u001b[39m._async_writer.write(event.SerializeToString())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/sbi_env/lib/python3.13/site-packages/tensorboard/summary/writer/event_file_writer.py:171\u001b[39m, in \u001b[36m_AsyncWriter.write\u001b[39m\u001b[34m(self, bytestring)\u001b[39m\n\u001b[32m    166\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Enqueue the given bytes to be written asychronously.\"\"\"\u001b[39;00m\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# Status of the worker should be checked under the lock to avoid\u001b[39;00m\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# multiple threads passing the check and then switching just before\u001b[39;00m\n\u001b[32m    170\u001b[39m     \u001b[38;5;66;03m# blocking on putting to the queue which might result in a deadlock.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_worker_status()\n\u001b[32m    172\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._closed:\n\u001b[32m    173\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mWriter is closed\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/sbi_env/lib/python3.13/site-packages/tensorboard/summary/writer/event_file_writer.py:212\u001b[39m, in \u001b[36m_AsyncWriter._check_worker_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    210\u001b[39m exception = \u001b[38;5;28mself\u001b[39m._worker.exception\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/sbi_env/lib/python3.13/threading.py:1041\u001b[39m, in \u001b[36mThread._bootstrap_inner\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1038\u001b[39m     _sys.setprofile(_profile_hook)\n\u001b[32m   1040\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1041\u001b[39m     \u001b[38;5;28mself\u001b[39m.run()\n\u001b[32m   1042\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m   1043\u001b[39m     \u001b[38;5;28mself\u001b[39m._invoke_excepthook(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/sbi_env/lib/python3.13/site-packages/tensorboard/summary/writer/event_file_writer.py:244\u001b[39m, in \u001b[36m_AsyncWriterThread.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m         \u001b[38;5;28mself\u001b[39m._run()\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m    246\u001b[39m         \u001b[38;5;28mself\u001b[39m.exception = ex\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/sbi_env/lib/python3.13/site-packages/tensorboard/summary/writer/event_file_writer.py:275\u001b[39m, in \u001b[36m_AsyncWriterThread._run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    273\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown_signal:\n\u001b[32m    274\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m     \u001b[38;5;28mself\u001b[39m._record_writer.write(data)\n\u001b[32m    276\u001b[39m     \u001b[38;5;28mself\u001b[39m._has_pending_data = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m queue.Empty:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/sbi_env/lib/python3.13/site-packages/tensorboard/summary/writer/record_writer.py:40\u001b[39m, in \u001b[36mRecordWriter.write\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m     38\u001b[39m header_crc = struct.pack(\u001b[33m\"\u001b[39m\u001b[33m<I\u001b[39m\u001b[33m\"\u001b[39m, masked_crc32c(header))\n\u001b[32m     39\u001b[39m footer_crc = struct.pack(\u001b[33m\"\u001b[39m\u001b[33m<I\u001b[39m\u001b[33m\"\u001b[39m, masked_crc32c(data))\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28mself\u001b[39m._writer.write(header + header_crc + data + footer_crc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/sbi_env/lib/python3.13/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py:775\u001b[39m, in \u001b[36mGFile.write\u001b[39m\u001b[34m(self, file_content)\u001b[39m\n\u001b[32m    771\u001b[39m         \u001b[38;5;28mself\u001b[39m.write_started = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    773\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    774\u001b[39m         \u001b[38;5;66;03m# append the later chunks\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m775\u001b[39m         \u001b[38;5;28mself\u001b[39m.fs.append(\u001b[38;5;28mself\u001b[39m.filename, file_content, \u001b[38;5;28mself\u001b[39m.binary_mode)\n\u001b[32m    776\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    777\u001b[39m     \u001b[38;5;66;03m# add to temp file, but wait for flush to write to final filesystem\u001b[39;00m\n\u001b[32m    778\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.write_temp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/sbi_env/lib/python3.13/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py:167\u001b[39m, in \u001b[36mLocalFileSystem.append\u001b[39m\u001b[34m(self, filename, file_content, binary_mode)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mappend\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename, file_content, binary_mode=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    160\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Append string file contents to a file.\u001b[39;00m\n\u001b[32m    161\u001b[39m \n\u001b[32m    162\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    165\u001b[39m \u001b[33;03m        binary_mode: bool, write as binary if True, otherwise text\u001b[39;00m\n\u001b[32m    166\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[38;5;28mself\u001b[39m._write(filename, file_content, \u001b[33m\"\u001b[39m\u001b[33mab\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m binary_mode \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33ma\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/sbi_env/lib/python3.13/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py:171\u001b[39m, in \u001b[36mLocalFileSystem._write\u001b[39m\u001b[34m(self, filename, file_content, mode)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_write\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename, file_content, mode):\n\u001b[32m    170\u001b[39m     encoding = \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mutf8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m io.open(filename, mode, encoding=encoding) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    172\u001b[39m         compatify = compat.as_bytes \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;28;01melse\u001b[39;00m compat.as_text\n\u001b[32m    173\u001b[39m         f.write(compatify(file_content))\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: b'/home/users/ids29/DGRB Scripts/Poisson Model/sbi-logs/NPE_C/2025-05-29T18_51_37.825037/events.out.tfevents.1748501497.kerr.1356725.2'"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------ #\n",
    "# 5.  Initialise\n",
    "# ------------------------------------------------------------------ #\n",
    "prior     = utils.BoxUniform(LOW, HIGH, device=\"cpu\")   # cpu for sampling speed\n",
    "with open('test_photon_info_Poisson_SFG1_part1.pkl', 'rb') as f:\n",
    "    raw_o = pickle.load(f) # raw photon data for test case\n",
    "x_o_full  = compute_summary_statistics(raw_o)       # 1-D tensor  (D_full,)\n",
    "# print(f\"x_o_full = {x_o_full}\")\n",
    "\n",
    "\n",
    "θ_batch = torch.load('thetas_training_Poisson_SFG1.pt') # thetas used for training dataset\n",
    "with open('training_photon_info_list_Poisson_SFG1.pkl', 'rb') as f:\n",
    "    raw_training = pickle.load(f) # raw photon data for training dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Θ_accum, X_accum = [], []           # running simulation database\n",
    "proposal = prior                    # start broad\n",
    "plt.ion()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# resume_flag = False   # Nothing to resume yet\n",
    "\n",
    "\n",
    "\n",
    "for r in range(1, NUM_ROUNDS + 1):\n",
    "    print(f\"\\n──────────  ROUND {r}/{NUM_ROUNDS}  ──────────\")\n",
    "\n",
    "    # 5.1  Draw parameters\n",
    "    # if r == 1:\n",
    "    #     θ_batch = prior.sample((SIMS_PER_ROUND,))    # (N,2) cpu\n",
    "    # else:\n",
    "    #     θ_batch = proposal.sample((SIMS_PER_ROUND,), x=x_o_z).cpu()\n",
    "\n",
    "    # 5.2  Simulate & summarise\n",
    "    # summaries = []\n",
    "    # for θ in θ_batch:\n",
    "    #     raw   = simulate_raw_photon_data(θ)\n",
    "    #     summ  = compute_summary_statistics(raw)\n",
    "    #     summaries.append(summ)\n",
    "    # x_batch_full = torch.stack(summaries)            # (N, D_full)\n",
    "    \n",
    "    # def simulate_and_summarize(theta):\n",
    "    #     raw  = simulate_raw_photon_data(theta)\n",
    "    #     return compute_summary_statistics(raw)\n",
    "\n",
    "    # dispatch 48 parallel jobs\n",
    "    summaries = Parallel(\n",
    "        n_jobs=num_workers,           # number of worker processes\n",
    "        backend=\"loky\",      # default multiprocess backend\n",
    "        verbose=10           # progress output\n",
    "    )(\n",
    "        delayed(compute_summary_statistics)(raw) \n",
    "        for raw in raw_training\n",
    "    )\n",
    "    print(\"Completed computed summaries\")\n",
    "\n",
    "    # stack back into a tensor\n",
    "    x_batch_full = torch.stack(summaries)  # shape: (SIMS_PER_ROUND, D_full)\n",
    "\n",
    "    # 5.3  Append to global store\n",
    "    Θ_accum.append(θ_batch)\n",
    "    X_accum.append(x_batch_full)\n",
    "    Θ_all = torch.cat(Θ_accum, 0)                    # (N_tot,2)\n",
    "    X_all_full = torch.cat(X_accum, 0)               # (N_tot,D_full)\n",
    "\n",
    "\n",
    "    X_z = X_all_full\n",
    "    x_o_z = x_o_full\n",
    "\n",
    "    \n",
    "\n",
    "    # 5.6  Build / update inference object\n",
    "    if r == 1:\n",
    "        # network\n",
    "        net = posterior_nn(model=\"nsf\", hidden_features=64, num_transforms=4,\n",
    "                           dropout_probability=0.4, use_combined_loss=True,\n",
    "                           z_score_x=\"none\", z_score_theta=\"none\")\n",
    "        inf = SNPE(prior=prior)\n",
    "        \n",
    "\n",
    "\n",
    "    inf.append_simulations(\n",
    "            θ_batch.to(device),        # θ from this round only\n",
    "            X_z.to(device),      # summaries from this round only\n",
    "    )\n",
    "\n",
    "\n",
    "    # inf.append_simulations(Θ_all.to(device), X_z.to(device))\n",
    "\n",
    "    de = inf.train()\n",
    "\n",
    "    # de = inf.train(\n",
    "    #     training_batch_size=256, learning_rate=1e-3,\n",
    "    #     validation_fraction=0.2, stop_after_epochs=50, max_num_epochs=500,\n",
    "    #     show_train_summary=False, clip_max_norm=1.0,\n",
    "    #     optimizer_class=AdamW,\n",
    "    #     optimizer_kwargs={\"lr\": 1e-3, \"weight_decay\": 5e-5})\n",
    "    \n",
    "    # resume_flag = True   # from now on the attribute exists\n",
    "\n",
    "    # 5.7  Loss curves\n",
    "    hist = inf._summary\n",
    "    fig_loss = plt.figure(figsize=(4,3))\n",
    "    plt.plot(hist[\"training_loss\"],   label=\"Train\")\n",
    "    plt.plot(hist[\"validation_loss\"], label=\"Val\")\n",
    "    plt.xlabel(\"epoch\"); plt.ylabel(\"−log prob\"); plt.legend()\n",
    "    plt.title(f\"Loss – round {r}\")\n",
    "    plt.show()\n",
    "\n",
    "    # 5.8  Posterior & sampling\n",
    "    posterior = inf.build_posterior(de)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    parameter_names = ['Poisson','SFG']\n",
    "    # parameter_labels = ['']\n",
    "    samples = posterior.sample((10000,), x=x_o_z) #100000\n",
    "    ranges = dict(zip(parameter_names, np.array(parameter_range).T.reshape((np.array(parameter_range[0]).size,2))))\n",
    "    mcsamples = MCSamples(samples=samples.numpy(), names = parameter_names, labels = LABELS, ranges = ranges)\n",
    "\n",
    "    g = plots.get_subplot_plotter()\n",
    "    g.settings.title_limit_fontsize = 16 # reference size for 3.5 inch subplot\n",
    "    g.settings.axes_fontsize=18\n",
    "    g.settings.legend_fontsize = 26\n",
    "    g.settings.axes_labelsize = 22\n",
    "    g.settings.norm_1d_density = True\n",
    "    g.settings.title_limit = 1\n",
    "    g.triangle_plot(mcsamples, filled=True, markers=theta_test.numpy(), param_limits=ranges)\n",
    "    # g.plot_1d([mcsamples], ['Source1'], filled=True)\n",
    "\n",
    "\n",
    "print(\"\\nAll rounds finished.  Close the figures to exit.\")\n",
    "plt.ioff()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

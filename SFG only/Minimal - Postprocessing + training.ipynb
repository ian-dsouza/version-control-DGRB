{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b18cda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7472a842-c503-4285-9818-7af9e069268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/users/ids29/DGRB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc88afbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aegis\n",
    "import numpy as np\n",
    "import healpy as hp\n",
    "import torch\n",
    "import pickle as pk\n",
    "from astropy import units as u\n",
    "from astropy import constants as c\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "import os\n",
    "from sbi.inference import SNPE #, prepare_for_sbi #, simulate_for_sbi\n",
    "from sbi import utils as utils\n",
    "from sbi import analysis as analysis\n",
    "# from sbi.inference.base import infer\n",
    "from getdist import plots, MCSamples\n",
    "import pickle\n",
    "from scipy.stats import norm, skew, kurtosis, spearmanr\n",
    "from scipy.integrate import quad, simpson\n",
    "from joblib import Parallel, delayed\n",
    "from torch.optim import AdamW\n",
    "from sbi.neural_nets import posterior_nn\n",
    "\n",
    "torch.set_printoptions(precision=10, sci_mode=True)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87cebaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOW: tensor([-4]); HIGH: tensor([-1])\n",
      "Teta_test = tensor([-2.0799999237e+00])\n",
      "Completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2309422/654387412.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  prior_param_range = torch.load('theta_range_SFG1.pt')\n",
      "/tmp/ipykernel_2309422/654387412.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  theta_test = torch.load('test_theta_SFG1_part1.pt')\n"
     ]
    }
   ],
   "source": [
    "grains=1000\n",
    "num_workers = 48  # or your chosen parallel worker count\n",
    "\n",
    "NUM_ROUNDS     = 1\n",
    "SIMS_PER_ROUND = int(1e3)          # Change only if the script that generates the data uses a different number of simulations\n",
    "POST_SAMPLES   = 10_000            # how many samples for corner plot\n",
    "\n",
    "prior_param_range = torch.load('theta_range_SFG1.pt')\n",
    "\n",
    "LOW  =   prior_param_range[0,:]\n",
    "HIGH =   prior_param_range[1,:]\n",
    "print(f\"LOW: {LOW}; HIGH: {HIGH}\")\n",
    "LABELS = [r\"\\log_{10}\\Phi_{\\rm SFG}\"]\n",
    "\n",
    "theta_test = torch.load('test_theta_SFG1_part1.pt')\n",
    "print(f\"Teta_test = {theta_test}\")\n",
    "\n",
    "use_energy_info = False\n",
    "\n",
    "print('Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32638c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n"
     ]
    }
   ],
   "source": [
    "parameter_range = [[], []]\n",
    "abundance_luminosity_and_spectrum_list = []\n",
    "source_class_list = []\n",
    "parameter_names = []\n",
    "energy_range = [1000, 100000] #MeV\n",
    "energy_range_gen = [energy_range[0]*0.5, energy_range[1]*18]\n",
    "max_radius = 8.5 + 20*2 #kpc\n",
    "exposure = 2000*10*0.2 #cm^2 yr\n",
    "flux_cut = 1e-9 #photons/cm^2/s\n",
    "angular_cut = np.pi #10*u.deg.to('rad') #degrees\n",
    "angular_cut_gen = np.pi #angular_cut*1.5\n",
    "lat_cut = 0 #2*u.deg.to('rad') #degrees\n",
    "lat_cut_gen = lat_cut*0.5\n",
    "\n",
    "\n",
    "print('Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e425ee8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n"
     ]
    }
   ],
   "source": [
    "my_cosmology = 'Planck18'\n",
    "z_range = [0, 14]\n",
    "luminosity_range = 10.0**np.array([37, 50]) # Minimum value set by considering Andromeda distance using Fermi as benchmark and receiving 0.1 photon at detector side\n",
    "my_AEGIS = aegis.aegis(abundance_luminosity_and_spectrum_list, source_class_list, parameter_range, energy_range, luminosity_range, max_radius, exposure, angular_cut, lat_cut, flux_cut, energy_range_gen=energy_range_gen, cosmology = my_cosmology, z_range = z_range, verbose = False)\n",
    "my_AEGIS.angular_cut_gen, my_AEGIS.lat_cut_gen = angular_cut_gen, lat_cut_gen\n",
    "\n",
    "print('Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51fe0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n"
     ]
    }
   ],
   "source": [
    "def compute_moments(values):\n",
    "    \"\"\"Compute the mean and variance of the energies.\"\"\"\n",
    "    mean = np.mean(values)\n",
    "    var = np.var(values)\n",
    "    skewness = skew(values)\n",
    "    kurt = kurtosis(values)\n",
    "    return mean, var, skewness, kurt\n",
    "\n",
    "def compute_quantiles(values, quantiles=[10, 25, 50, 75, 90]):\n",
    "    \"\"\"\n",
    "    Compute the specified quantiles (in percent).\n",
    "    For example, the 25th quantile is the energy such that 25% of the data lies below it.\n",
    "    Returns a dictionary mapping percentiles to values.\n",
    "    \"\"\"\n",
    "    q_values = np.percentile(values, quantiles)\n",
    "    return dict(zip(quantiles, q_values))\n",
    "\n",
    "def normalize_quantiles(q_dict, val_min, val_max):\n",
    "    \"\"\"\n",
    "    Normalize quantile values from a dictionary using min-max normalization.\n",
    "    \"\"\"\n",
    "    norm_q = {perc: (val - val_min) / (val_max - val_min) for perc, val in q_dict.items()}\n",
    "    return norm_q\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def effective_spectral_index(energies, E_lower):\n",
    "    \"\"\"\n",
    "    Compute effective spectral index via MLE.\n",
    "    Returns np.nan if there are no photons.\n",
    "    \"\"\"\n",
    "    energies = np.array(energies)\n",
    "    n = len(energies)\n",
    "    if n == 0:\n",
    "        return np.nan\n",
    "    sum_logs = np.sum(np.log(energies / E_lower))\n",
    "    return 1 + n / sum_logs\n",
    "\n",
    "def smooth_binned_gamma(binned_gamma, counts, global_gamma, min_count=6):\n",
    "    \"\"\"\n",
    "    Smooth binned effective spectral indices if the count in a bin is low.\n",
    "    \n",
    "    Parameters:\n",
    "      binned_gamma : array-like\n",
    "          Array of spectral indices per energy bin (may contain np.nan for empty bins).\n",
    "      counts : array-like\n",
    "          Array of photon counts per bin.\n",
    "      global_gamma : float\n",
    "          Global effective spectral index, used as fallback.\n",
    "      min_count : int\n",
    "          Minimum photon count required to trust a bin.\n",
    "    \n",
    "    Returns:\n",
    "      smoothed_gamma : numpy array with smoothed spectral index values.\n",
    "    \"\"\"\n",
    "    binned_gamma = np.array(binned_gamma)\n",
    "    counts = np.array(counts)\n",
    "    num_bins = len(binned_gamma)\n",
    "    smoothed_gamma = binned_gamma.copy()\n",
    "\n",
    "    # Loop over each bin.\n",
    "    for i in range(num_bins):\n",
    "        if counts[i] < min_count or np.isnan(binned_gamma[i]):\n",
    "            indices = [i]\n",
    "            if i > 0:\n",
    "                indices.append(i-1)\n",
    "            if i < num_bins - 1:\n",
    "                indices.append(i+1)\n",
    "            indices = list(set(indices))  # ensure uniqueness\n",
    "            \n",
    "            # Only include indices with non-NaN values\n",
    "            valid_indices = [j for j in indices if not np.isnan(binned_gamma[j])]\n",
    "            if valid_indices:\n",
    "                # Use the counts as weights. If any count is zero, it's fine; it will reduce the weight.\n",
    "                weights = np.array([counts[j] for j in valid_indices], dtype=np.float32)\n",
    "                # If all weights sum to 0, default to global_gamma.\n",
    "                if np.sum(weights) > 0:\n",
    "                    smoothed_gamma[i] = np.average(binned_gamma[valid_indices], weights=weights)\n",
    "                else:\n",
    "                    smoothed_gamma[i] = global_gamma\n",
    "            else:\n",
    "                # If none of the neighboring bins have a valid estimate, fallback to global_gamma.\n",
    "                smoothed_gamma[i] = global_gamma\n",
    "    return smoothed_gamma\n",
    "\n",
    "def compute_binned_effective_spectral_indices(energies, num_bins, energy_range):\n",
    "    \"\"\"\n",
    "    Divide the energy range into logarithmic bins and compute the effective spectral index in each bin.\n",
    "    For bins with few photons (count < min_count) or empty bins, apply smoothing by averaging with neighboring bins.\n",
    "    \n",
    "    Returns:\n",
    "      smoothed_gamma : 1D array containing the (possibly smoothed) spectral index for each bin.\n",
    "      global_gamma  : Effective spectral index computed using all energies (with the lowest bin edge as E_lower).\n",
    "      counts        : Raw photon counts per bin.\n",
    "    \"\"\"\n",
    "    bins = np.geomspace(energy_range[0], energy_range[1], num_bins + 1)\n",
    "    binned_gamma = []\n",
    "    counts = []  # photon counts per bin\n",
    "\n",
    "    # Loop over bins\n",
    "    for i in range(len(bins) - 1):\n",
    "        mask = (energies >= bins[i]) & (energies < bins[i+1])\n",
    "        energies_bin = energies[mask]\n",
    "        counts.append(len(energies_bin))\n",
    "        # Compute gamma for the bin; if the bin is empty, effective_spectral_index returns np.nan.\n",
    "        gamma = effective_spectral_index(energies_bin, E_lower=bins[i])\n",
    "        binned_gamma.append(gamma)\n",
    "    \n",
    "    binned_gamma = np.array(binned_gamma)\n",
    "    counts = np.array(counts)\n",
    "    \n",
    "    # Compute the global effective spectral index over all energies using the first bin's lower edge.\n",
    "    global_gamma = effective_spectral_index(energies, E_lower=bins[0])\n",
    "    \n",
    "    # Smooth the binned_gamma using neighboring bins if counts in a bin are low.\n",
    "    smoothed_gamma = smooth_binned_gamma(binned_gamma, counts, global_gamma, min_count=6)\n",
    "    \n",
    "    return smoothed_gamma, global_gamma, counts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def normalize_binned_gamma(binned_gamma, global_gamma, mode='ratio'):\n",
    "    \"\"\"\n",
    "    Normalize the binned effective spectral indices relative to the global effective spectral index.\n",
    "    \n",
    "    Parameters:\n",
    "      binned_gamma : np.array\n",
    "          Array of effective spectral indices for each energy bin.\n",
    "      global_gamma : float\n",
    "          The global effective spectral index computed over the entire energy range.\n",
    "      mode : str, optional\n",
    "          'ratio' for normalized_binned_gamma = binned_gamma / global_gamma,\n",
    "          'difference' for normalized_binned_gamma = (binned_gamma - global_gamma)/global_gamma.\n",
    "          Default is 'ratio'.\n",
    "    \n",
    "    Returns:\n",
    "      normalized_binned_gamma : np.array\n",
    "    \"\"\"\n",
    "    if mode == 'ratio':\n",
    "        # Return ratio relative to global spectral index\n",
    "        normalized_binned_gamma = binned_gamma / global_gamma\n",
    "    elif mode == 'difference':\n",
    "        # Return relative differences with baseline 0\n",
    "        normalized_binned_gamma = (binned_gamma - global_gamma) / global_gamma\n",
    "    else:\n",
    "        raise ValueError(\"Unknown mode for normalization. Choose 'ratio' or 'difference'.\")\n",
    "    return normalized_binned_gamma\n",
    "\n",
    "\n",
    "# --- concise, log-binned 1-D energy histogram ---------------------------\n",
    "def compute_energy_only_histogram(energies, num_bins, energy_range=(1000, 100_000)):\n",
    "    \"\"\"\n",
    "    Log-bin photon energies (MeV) into `num_bins` and return raw counts.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    energies      : 1-D iterable (list / np.ndarray / torch.Tensor)\n",
    "    num_bins      : int, number of logarithmic bins\n",
    "    energy_range  : (low, high) edges in MeV  (default 1–100 GeV)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    hist : 1-D np.ndarray length `num_bins`  (integer counts)\n",
    "    \"\"\"\n",
    "    edges = np.geomspace(energy_range[0], energy_range[1], num_bins + 1, dtype=np.float32)\n",
    "    hist, _ = np.histogram(np.asarray(energies, dtype=np.float32), bins=edges)\n",
    "    return hist.astype(np.float32)\n",
    "\n",
    "\n",
    "def normalize_energy_only_hist(energy_only_hist):\n",
    "    \"\"\"\n",
    "    Normalize a 1D histogram so it sums to 1.\n",
    "    \"\"\"\n",
    "    total = np.sum(energy_only_hist) # this is the total number of photons in the energy-only histogram\n",
    "    if total == 0:\n",
    "        return energy_only_hist  # or handle the zero-case as needed\n",
    "    return energy_only_hist / total\n",
    "\n",
    "# def compute_sub_statistics(energies, counts, N_Ebins):\n",
    "#     \"\"\"\n",
    "#     Given an array of photon energies (all between 1 and 100 GeV), compute a set of summary statistics:\n",
    "#       1. Mean energy.\n",
    "#       2. Variance of energy.\n",
    "#       3. Quantiles: 10%, 25%, 50%, 75%, and 90%.\n",
    "#       4. Effective spectral index estimated from the data.\n",
    "      \n",
    "#     Returns the statistics in a dictionary and also a flattened torch tensor.\n",
    "#     \"\"\"\n",
    "#     # 1. Mean and variance\n",
    "#     mean_E, var_E, skewness_E, kurt_E  = compute_moments(energies)\n",
    "#     mean_counts, var_counts, skewness_counts, kurt_counts  = compute_moments(counts)\n",
    "    \n",
    "#     # 2. Quantiles\n",
    "#     quant_dict_E = compute_quantiles(energies)  # This returns a dict like {10: val, 25: val, ...}\n",
    "#     norm_quant_dict_E = normalize_quantiles(quant_dict_E, val_min=energy_range[0], val_max=energy_range[1])\n",
    "\n",
    "#     quant_dict_counts = compute_quantiles(counts)  # This returns a dict like {10: val, 25: val, ...}\n",
    "#     norm_quant_dict_counts = normalize_quantiles(quant_dict_counts, val_min=0, val_max=np.max(counts))\n",
    "\n",
    "#     energy_only_hist = compute_energy_only_histogram(energies, num_bins=N_Ebins, energy_range=energy_range)\n",
    "\n",
    "#     binned_gamma, global_gamma, energy_only_hist = compute_binned_effective_spectral_indices(energies, num_bins=N_Ebins, energy_range=energy_range)\n",
    "#     norm_binned_gamma = normalize_binned_gamma(binned_gamma, global_gamma, mode='ratio')\n",
    "#     norm_energy_only_hist = normalize_energy_only_hist(energy_only_hist)\n",
    "    \n",
    "#     # If you want to pass the summary statistic to sbi, it is best to use a fixed-size vector (e.g., a torch tensor).\n",
    "#     # For example, arrange the stats in a consistent order:\n",
    "#     scalars  = np.array([\n",
    "#         energies.size, # total number of photons\n",
    "#         mean_E, var_E , skewness_E, kurt_E,\n",
    "#         mean_counts, var_counts, skewness_counts, kurt_counts,\n",
    "#         global_gamma\n",
    "#     ], dtype=np.float32)\n",
    "\n",
    "    \n",
    "#     return scalars , norm_quant_dict_E, norm_quant_dict_counts, norm_binned_gamma, norm_energy_only_hist\n",
    "\n",
    "def compute_sub_statistics(energies, counts, N_Ebins):\n",
    "    \"\"\"\n",
    "    Given an array of photon energies (all between 1 and 100 GeV), compute a set of summary statistics:\n",
    "      1. Mean energy.\n",
    "      2. Variance of energy.\n",
    "      3. Quantiles: 10%, 25%, 50%, 75%, and 90%.\n",
    "      4. Effective spectral index estimated from the data.\n",
    "      \n",
    "    Returns the statistics in a dictionary and also a flattened torch tensor.\n",
    "    \"\"\"\n",
    "    # 1. Mean and variance\n",
    "    mean_E, var_E, skewness_E, kurt_E  = compute_moments(energies)\n",
    "    mean_counts, var_counts, skewness_counts, kurt_counts  = compute_moments(counts)\n",
    "    \n",
    "    # 2. Quantiles\n",
    "    quant_dict_E = compute_quantiles(energies)  # This returns a dict like {10: val, 25: val, ...}\n",
    "    norm_quant_dict_E = normalize_quantiles(quant_dict_E, val_min=energy_range[0], val_max=energy_range[1])\n",
    "\n",
    "    quant_dict_counts = compute_quantiles(counts)  # This returns a dict like {10: val, 25: val, ...}\n",
    "    norm_quant_dict_counts = normalize_quantiles(quant_dict_counts, val_min=0, val_max=np.max(counts))\n",
    "\n",
    "    energy_only_hist = compute_energy_only_histogram(energies, num_bins=N_Ebins, energy_range=energy_range)\n",
    "\n",
    "    binned_gamma, global_gamma, energy_only_hist = compute_binned_effective_spectral_indices(energies, num_bins=N_Ebins, energy_range=energy_range)\n",
    "    norm_binned_gamma = normalize_binned_gamma(binned_gamma, global_gamma, mode='ratio')\n",
    "    norm_energy_only_hist = normalize_energy_only_hist(energy_only_hist)\n",
    "    \n",
    "    # If you want to pass the summary statistic to sbi, it is best to use a fixed-size vector (e.g., a torch tensor).\n",
    "    # For example, arrange the stats in a consistent order:\n",
    "    energy_only_scalars  = np.array([\n",
    "        mean_E, var_E , skewness_E, kurt_E,\n",
    "        global_gamma\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    other_scalars  = np.array([\n",
    "        energies.size, #total number of photons\n",
    "        mean_counts, var_counts, skewness_counts, kurt_counts,\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    \n",
    "    # USES NORMALIZED SUMMARIES\n",
    "    # return energy_only_scalars, other_scalars, norm_quant_dict_E, norm_quant_dict_counts, norm_binned_gamma, norm_energy_only_hist\n",
    "\n",
    "    # USES UNNORMALIZED SUMMARIES\n",
    "    return energy_only_scalars, other_scalars, quant_dict_E, quant_dict_counts, binned_gamma, energy_only_hist\n",
    "\n",
    "def normalize_energy_dependent_hist(energy_dependent_hist, total_pixels):\n",
    "    \"\"\"\n",
    "    Normalize a 2D histogram so that each column sums to 1.\n",
    "    total_pixels: the fixed number of pixels (for example, len(roi_pix_i))\n",
    "    \"\"\"\n",
    "    # If the histogram is of shape (N_countbins, N_Ebins),\n",
    "    # each column should ideally sum to total_pixels.\n",
    "    normalized_hist = energy_dependent_hist / total_pixels\n",
    "    return normalized_hist\n",
    "\n",
    "def fano_from_photons(photon_info, nsides=(32, 64, 128), center_mask_deg=10, lat_mask_deg=5): #Outputs an array of Fano factors for each NSIDE (currently three in number).\n",
    "    \"\"\"\n",
    "    photon_info must expose `theta` (co-latitude, rad) and `phi` (longitude, rad).\n",
    "    Returns ΔF for each NSIDE in `nsides`.\n",
    "    \"\"\"\n",
    "    # theta_p = photon_info['theta']   # or however your struct stores it\n",
    "    # phi_p   = photon_info['phi']\n",
    "\n",
    "    deltaF = []\n",
    "    for N_side in nsides:\n",
    "\n",
    "        N_pix = 12*N_side**2\n",
    "        pix_i = np.linspace(0, N_pix-1, N_pix, dtype = 'int')\n",
    "        roi_pix_i = np.where(np.logical_and(hp.rotator.angdist(np.array([np.pi/2, 0]), hp.pix2ang(N_side, pix_i)) >= center_mask_deg*u.deg.to('rad'), np.abs(np.pi/2 - hp.pix2ang(N_side, pix_i)[0]) >= lat_mask_deg*u.deg.to('rad')))[0]\n",
    "\n",
    "        N_Ebins = 1 # This value doesn't matter because in 'get_roi_map_summary' fucntion, when you use 'Ebinspace = single', it will only consider one energy bin, and not use the N_Ebins value.\n",
    "        roi_map_1D = my_AEGIS.get_roi_map_summary(photon_info = photon_info, N_side = N_side, N_Ebins = N_Ebins, Ebinspace = 'single', roi_pix_i = roi_pix_i) # 'single' means only one energy bin.\n",
    "        counts = roi_map_1D.ravel()\n",
    "\n",
    "        #Fano excess\n",
    "        mu  = counts.mean()\n",
    "        var = counts.var()\n",
    "        deltaF.append(max(0.0, var/mu - 1.0) if mu > 0 else 0.0)\n",
    "\n",
    "    return np.asarray(deltaF, dtype=np.float32)\n",
    "\n",
    "\n",
    "\n",
    "print('Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "908cfbb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n"
     ]
    }
   ],
   "source": [
    "# a simple simulator with the total number of photons as the summary statistic\n",
    "def compute_summary_statistics(photon_info):\n",
    "\n",
    "    N_side = 2**6\n",
    "    #parameters for the summary statistic\n",
    "    \n",
    "    center_mask = 10 #deg \n",
    "    lat_mask = 5 #deg \n",
    "    N_Ebins = 20\n",
    "    # Ebinspace = 'log'#'linear'\n",
    "    N_countbins = 10\n",
    "    countbinspace = 'custom'#'linear'\n",
    "    mincount_2D_hist, maxcount_2D_hist = 0, 38 # For energy-dependent 2D histogram # Change for different data sets. Consider the data set with the maximum number of photons to decide the value of 'maxcount'.\n",
    "    mincount_1D_counts_hist, maxcount_1D_counts_hist = 0, 123 # For counts only 1D histogram # Change for different data sets. Consider the data set with the maximum number of photons to decide the value of 'maxcount'.\n",
    "    N_pix = 12*N_side**2\n",
    "    pix_i = np.linspace(0, N_pix-1, N_pix, dtype = 'int')\n",
    "    roi_pix_i = np.where(np.logical_and(hp.rotator.angdist(np.array([np.pi/2, 0]), hp.pix2ang(N_side, pix_i)) >= center_mask*u.deg.to('rad'), np.abs(np.pi/2 - hp.pix2ang(N_side, pix_i)[0]) >= lat_mask*u.deg.to('rad')))[0]\n",
    "\n",
    "    # Get energy dependent 2D histogram\n",
    "    roi_map_2D = my_AEGIS.get_roi_map_summary(photon_info = photon_info, N_side = N_side, N_Ebins = N_Ebins, Ebinspace = 'log', roi_pix_i = roi_pix_i)\n",
    "    # print(f\"For energy-dependent hist, max counts = {np.max(roi_map)}; roi_map.shape = {roi_map.shape}\")\n",
    "    energy_dependent_hist = my_AEGIS.get_counts_histogram_from_roi_map(roi_map_2D, mincount = mincount_2D_hist, maxcount = maxcount_2D_hist, N_countbins = N_countbins, countbinspace = countbinspace)\n",
    "    total_pixels = roi_pix_i.size\n",
    "    norm_energy_dependent_hist = normalize_energy_dependent_hist(energy_dependent_hist, total_pixels)\n",
    "\n",
    "    # Get counts-only 1D histogram\n",
    "    roi_map_1D = my_AEGIS.get_roi_map_summary(photon_info = photon_info, N_side = N_side, N_Ebins = N_Ebins, Ebinspace = 'single', roi_pix_i = roi_pix_i) # 'single' means only one energy bin.\n",
    "    # print(f\"For counts-only hist, max counts = {np.max(roi_map)}; roi_map.shape = {roi_map.shape}\")\n",
    "    counts_only_hist = my_AEGIS.get_counts_histogram_from_roi_map(roi_map_1D, mincount = mincount_1D_counts_hist, maxcount = maxcount_1D_counts_hist, N_countbins = N_countbins, countbinspace = countbinspace)\n",
    "    total_pixels = roi_pix_i.size\n",
    "    norm_counts_only_hist = normalize_energy_dependent_hist(counts_only_hist, total_pixels)\n",
    "\n",
    "\n",
    "    photon_pixels = hp.ang2pix(N_side, photon_info['angles'][:, 0], photon_info['angles'][:, 1])\n",
    "    roi_mask = np.isin(photon_pixels, roi_pix_i)\n",
    "    energies_in_roi = photon_info['energies'][roi_mask]\n",
    "\n",
    "    counts = roi_map_1D.ravel()\n",
    "    energy_only_scalars, other_scalars, norm_quant_dict_E, norm_quant_dict_counts, norm_binned_gamma, norm_energy_only_hist = compute_sub_statistics(energies_in_roi, counts, N_Ebins = N_Ebins) # Original code\n",
    "    \n",
    "    QUANTILES = [10, 25, 50, 75, 90]\n",
    "    sorted_keys_E = sorted(norm_quant_dict_E.keys())\n",
    "    assert sorted_keys_E == QUANTILES, \"Quantile keys differ from expected set\"\n",
    "    norm_quant_vector_E = np.array([norm_quant_dict_E[k] for k in sorted_keys_E], dtype=np.float32)\n",
    "\n",
    "    sorted_keys_counts = sorted(norm_quant_dict_counts.keys())\n",
    "    assert sorted_keys_counts == QUANTILES, \"Quantile keys differ from expected set\"\n",
    "    norm_quant_vector_counts = np.array([norm_quant_dict_counts[k] for k in sorted_keys_counts], dtype=np.float32)\n",
    "\n",
    "    flat_norm_energy_dependent_hist = np.asarray(norm_energy_dependent_hist, dtype=np.float32).flatten()\n",
    "    flat_norm_counts_only_hist = np.asarray(norm_counts_only_hist, dtype=np.float32).flatten()\n",
    "\n",
    "    fano_factors = fano_from_photons(photon_info, center_mask_deg=center_mask, lat_mask_deg=lat_mask) # evaluated at N-sides 32, 64, and 128\n",
    "\n",
    "    # USES NORMALIZED SUMMARIES\n",
    "\n",
    "    # if use_energy_info:\n",
    "    #     ## This is the full summary statistic array\n",
    "    #     parts = [          \n",
    "    #         energy_only_scalars,                                         # scalars that depend only on energy\n",
    "    #         other_scalars,                                              # scalars that don't depend on energy\n",
    "    #         np.array([np.max(counts)], dtype=np.float32), # max counts that any pixel sees\n",
    "    #         fano_factors,\n",
    "    #         norm_quant_vector_E,\n",
    "    #         norm_quant_vector_counts,\n",
    "    #         norm_binned_gamma,\n",
    "    #         norm_energy_only_hist,\n",
    "    #         flat_norm_counts_only_hist,\n",
    "    #         flat_norm_energy_dependent_hist\n",
    "    #     ]\n",
    "    # else:\n",
    "    #     ## This is the reduced summary statistic array, which excludes energy-only information\n",
    "    #     parts = [          \n",
    "    #         other_scalars,                                          # all features except the scalars\n",
    "    #         np.array([np.max(counts)], dtype=np.float32), # max counts that any pixel sees\n",
    "    #         fano_factors,\n",
    "    #         norm_quant_vector_counts,\n",
    "    #         flat_norm_counts_only_hist,\n",
    "    #         flat_norm_energy_dependent_hist\n",
    "    #     ]\n",
    "\n",
    "    # USES uNNORMALIZED SUMMARIES\n",
    "\n",
    "    # if use_energy_info:\n",
    "    #     ## This is the full summary statistic array\n",
    "    #     parts = [          \n",
    "    #         energy_only_scalars,                                         # scalars that depend only on energy\n",
    "    #         other_scalars,                                              # scalars that don't depend on energy\n",
    "    #         np.array([np.max(counts)], dtype=np.float32), # max counts that any pixel sees\n",
    "    #         fano_factors,\n",
    "    #         norm_quant_vector_E,\n",
    "    #         norm_quant_vector_counts,\n",
    "    #         norm_binned_gamma,\n",
    "    #         norm_energy_only_hist,\n",
    "    #         np.asarray(counts_only_hist, dtype=np.float32).flatten(),\n",
    "    #         np.asarray(energy_dependent_hist, dtype=np.float32).flatten()\n",
    "    #     ]\n",
    "    # else:\n",
    "    #     ## This is the reduced summary statistic array, which excludes energy-only information\n",
    "    #     parts = [          \n",
    "    #         other_scalars,                                          # all features except the scalars\n",
    "    #         np.array([np.max(counts)], dtype=np.float32), # max counts that any pixel sees\n",
    "    #         fano_factors,\n",
    "    #         norm_quant_vector_counts,\n",
    "    #         np.asarray(counts_only_hist, dtype=np.float32).flatten(),\n",
    "    #         np.asarray(energy_dependent_hist, dtype=np.float32).flatten()\n",
    "    #     ]\n",
    "\n",
    "\n",
    "    # TRYING TO REDUCE THE NUMBER OF SUMMARIES\n",
    "    parts = [\n",
    "        np.array([other_scalars[0]])\n",
    "    ]\n",
    "\n",
    "    summary_array = np.concatenate(parts, axis=0)\n",
    "    \n",
    "    return torch.as_tensor(summary_array, dtype=torch.float32)   # stays on CPU\n",
    "\n",
    "print('Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2e11c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of photons for test case withe log(Phi_SFG) = -2.0799999237060547 is 32464\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'compute_summary_statistics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m     raw_o = pickle.load(f) \u001b[38;5;66;03m# raw photon data for test case\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNumber of photons for test case withe log(Phi_SFG) = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtheta_test.numpy()[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mraw_o[\u001b[33m'\u001b[39m\u001b[33menergies\u001b[39m\u001b[33m'\u001b[39m].size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m x_o_full  = compute_summary_statistics(raw_o)       \u001b[38;5;66;03m# 1-D tensor  (D_full,)\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mx_o_full = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx_o_full\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m θ_batch = torch.load(\u001b[33m'\u001b[39m\u001b[33mtraining_theta_SFG1.pt\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;66;03m# thetas used for training dataset\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'compute_summary_statistics' is not defined"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------ #\n",
    "# 5.  Initialise\n",
    "# ------------------------------------------------------------------ #\n",
    "prior     = utils.BoxUniform(LOW, HIGH, device=\"cpu\")   # cpu for sampling speed\n",
    "with open('test_photon_info_SFG1_part1.pkl', 'rb') as f:\n",
    "    raw_o = pickle.load(f) # raw photon data for test case\n",
    "print(f\"Number of photons for test case withe log(Phi_SFG) = {theta_test.numpy()[0]} is {raw_o['energies'].size}\")\n",
    "x_o_full  = compute_summary_statistics(raw_o)       # 1-D tensor  (D_full,)\n",
    "print(f\"x_o_full = {x_o_full}\")\n",
    "\n",
    "\n",
    "θ_batch = torch.load('training_theta_SFG1.pt') # thetas used for training dataset\n",
    "with open('training_photon_info_SFG1.pkl', 'rb') as f:\n",
    "    raw_training = pickle.load(f) # raw photon data for training dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Θ_accum, X_accum = [], []           # running simulation database\n",
    "proposal = prior                    # start broad\n",
    "plt.ion()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# resume_flag = False   # Nothing to resume yet\n",
    "\n",
    "\n",
    "\n",
    "for r in range(1, NUM_ROUNDS + 1):\n",
    "    print(f\"\\n──────────  ROUND {r}/{NUM_ROUNDS}  ──────────\")\n",
    "\n",
    "    # 5.1  Draw parameters\n",
    "    # if r == 1:\n",
    "    #     θ_batch = prior.sample((SIMS_PER_ROUND,))    # (N,2) cpu\n",
    "    # else:\n",
    "    #     θ_batch = proposal.sample((SIMS_PER_ROUND,), x=x_o_z).cpu()\n",
    "\n",
    "    # 5.2  Simulate & summarise\n",
    "    # summaries = []\n",
    "    # for θ in θ_batch:\n",
    "    #     raw   = simulate_raw_photon_data(θ)\n",
    "    #     summ  = compute_summary_statistics(raw)\n",
    "    #     summaries.append(summ)\n",
    "    # x_batch_full = torch.stack(summaries)            # (N, D_full)\n",
    "    \n",
    "    # def simulate_and_summarize(theta):\n",
    "    #     raw  = simulate_raw_photon_data(theta)\n",
    "    #     return compute_summary_statistics(raw)\n",
    "\n",
    "    # dispatch 48 parallel jobs\n",
    "    summaries = Parallel(\n",
    "        n_jobs=num_workers,           # number of worker processes\n",
    "        backend=\"loky\",      # default multiprocess backend\n",
    "        verbose=10           # progress output\n",
    "    )(\n",
    "        delayed(compute_summary_statistics)(raw) \n",
    "        for raw in raw_training\n",
    "    )\n",
    "\n",
    "    # stack back into a tensor\n",
    "    x_batch_full = torch.stack(summaries)  # shape: (SIMS_PER_ROUND, D_full)\n",
    "\n",
    "    # 5.3  Append to global store\n",
    "    Θ_accum.append(θ_batch)\n",
    "    X_accum.append(x_batch_full)\n",
    "    Θ_all = torch.cat(Θ_accum, 0)                    # (N_tot,2)\n",
    "    X_all_full = torch.cat(X_accum, 0)               # (N_tot,D_full)\n",
    "\n",
    "\n",
    "    X_z = X_all_full\n",
    "    x_o_z = x_o_full\n",
    "\n",
    "    \n",
    "\n",
    "    # 5.6  Build / update inference object\n",
    "    if r == 1:\n",
    "        # network\n",
    "        net = posterior_nn(model=\"nsf\", hidden_features=64, num_transforms=4,\n",
    "                           dropout_probability=0.4, use_combined_loss=True,\n",
    "                           z_score_x=\"none\", z_score_theta=\"none\")\n",
    "        inf = SNPE(prior=prior)\n",
    "        \n",
    "\n",
    "\n",
    "    inf.append_simulations(\n",
    "            θ_batch.to(device),        # θ from this round only\n",
    "            X_z.to(device),      # summaries from this round only\n",
    "    )\n",
    "\n",
    "\n",
    "    # inf.append_simulations(Θ_all.to(device), X_z.to(device))\n",
    "\n",
    "    de = inf.train()\n",
    "\n",
    "    # de = inf.train(\n",
    "    #     training_batch_size=256, learning_rate=1e-3,\n",
    "    #     validation_fraction=0.2, stop_after_epochs=50, max_num_epochs=500,\n",
    "    #     show_train_summary=False, clip_max_norm=1.0,\n",
    "    #     optimizer_class=AdamW,\n",
    "    #     optimizer_kwargs={\"lr\": 1e-3, \"weight_decay\": 5e-5})\n",
    "    \n",
    "    # resume_flag = True   # from now on the attribute exists\n",
    "\n",
    "    # 5.7  Loss curves\n",
    "    hist = inf._summary\n",
    "    fig_loss = plt.figure(figsize=(4,3))\n",
    "    plt.plot(hist[\"training_loss\"],   label=\"Train\")\n",
    "    plt.plot(hist[\"validation_loss\"], label=\"Val\")\n",
    "    plt.xlabel(\"epoch\"); plt.ylabel(\"−log prob\"); plt.legend()\n",
    "    plt.title(f\"Loss – round {r}\")\n",
    "    plt.show()\n",
    "\n",
    "    # 5.8  Posterior & sampling\n",
    "    posterior = inf.build_posterior(de)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    parameter_names = ['Source1']\n",
    "    parameter_labels = ['Abundance']\n",
    "    samples = posterior.sample((10000,), x=x_o_z) #100000\n",
    "    ranges = dict(zip(parameter_names, np.array(parameter_range).T.reshape((np.array(parameter_range[0]).size,2))))\n",
    "    mcsamples = MCSamples(samples=samples.numpy(), names = parameter_names, labels = parameter_labels, ranges = ranges)\n",
    "\n",
    "    g = plots.get_subplot_plotter()\n",
    "    g.settings.title_limit_fontsize = 16 # reference size for 3.5 inch subplot\n",
    "    g.settings.axes_fontsize=18\n",
    "    g.settings.legend_fontsize = 26\n",
    "    g.settings.axes_labelsize = 22\n",
    "    g.settings.norm_1d_density = True\n",
    "    g.settings.title_limit = 1\n",
    "    g.triangle_plot(mcsamples, filled=True, markers=theta_test.numpy(), param_limits=ranges)\n",
    "    # g.plot_1d([mcsamples], ['Source1'], filled=True)\n",
    "\n",
    "\n",
    "print(\"\\nAll rounds finished.  Close the figures to exit.\")\n",
    "plt.ioff()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbi_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
